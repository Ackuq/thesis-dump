{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afe8821",
   "metadata": {},
   "source": [
    "# Optimizations for PIT-joins\n",
    "\n",
    "This notebook will consist of several optimizations for the existing join method. Stuff that will be looked into is the unoptimized PIT-join as well as optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d292356",
   "metadata": {},
   "source": [
    "# 0. Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301f9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>36</td><td>application_1642582607798_0035</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1642582607798_0035/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e03_1642582607798_0035_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
      "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.expressions.Window\n",
      "import io.hops.util.Hops\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import io.hops.util.Hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c6298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import io.github.ackuq._\n"
     ]
    }
   ],
   "source": [
    "import io.github.ackuq._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855d2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(ts,IntegerType,false), StructField(label,StringType,false))\n",
      "fg2_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id_2,IntegerType,false), StructField(ts_2,IntegerType,false), StructField(f2,StringType,false))\n",
      "fg3_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id_3,IntegerType,false), StructField(ts_3,IntegerType,false), StructField(f3,StringType,false))\n"
     ]
    }
   ],
   "source": [
    "val fg1_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, false),\n",
    "  StructField(\"ts\", IntegerType, false),\n",
    "  StructField(\"label\", StringType, false)    \n",
    "))\n",
    "\n",
    "val fg2_schema = StructType(Array(\n",
    "  StructField(\"id_2\", IntegerType, false),\n",
    "  StructField(\"ts_2\", IntegerType, false),\n",
    "  StructField(\"f2\", StringType, false)    \n",
    "))\n",
    "\n",
    "val fg3_schema = StructType(Array(\n",
    "  StructField(\"id_3\", IntegerType, false),\n",
    "  StructField(\"ts_3\", IntegerType, false),\n",
    "  StructField(\"f3\", StringType, false)    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40bbb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@738e1931\n"
     ]
    }
   ],
   "source": [
    "val spark = SparkSession.builder().master(\"local\").appName(\"PIT Optimizations Scala\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe5259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use \" + Hops.getProjectName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf8b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data2: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data3: Seq[org.apache.spark.sql.Row] = List([1,1,f3-1-1], [2,2,f3-2-2], [1,6,f3-1-6], [2,8,f3-2-8], [1,10,f3-1-10])\n"
     ]
    }
   ],
   "source": [
    "val data1 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data2 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data3 = Seq(\n",
    "    Row(1, 1, \"f3-1-1\"),\n",
    "    Row(2, 2, \"f3-2-2\"),\n",
    "    Row(1, 6, \"f3-1-6\"),\n",
    "    Row(2, 8, \"f3-2-8\"),\n",
    "    Row(1, 10, \"f3-1-10\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693b9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val fg1 = spark.createDataFrame(spark.sparkContext.parallelize(data1), schema=fg1_schema) \n",
    "val fg2 = spark.createDataFrame(spark.sparkContext.parallelize(data2), schema=fg2_schema) \n",
    "val fg3 = spark.createDataFrame(spark.sparkContext.parallelize(data3), schema=fg3_schema) \n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c484d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val DATA_PATH = \"hdfs:///Projects/\" + Hops.getProjectName + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "\n",
    "val fg1 = spark.read.option(\"header\", true).schema(fg1_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg1.count()\n",
    "\n",
    "val fg2 = spark.read.option(\"header\", true).schema(fg2_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts_2\")).persist()\n",
    "fg2.count()\n",
    "\n",
    "val fg3 = spark.read.option(\"header\", true).schema(fg3_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts_3\")).persist()\n",
    "fg3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56419249",
   "metadata": {},
   "source": [
    "# 1. Regular PIT-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitJoin(joinHint: String, labelData: DataFrame, fgs: DataFrame*) : DataFrame = {\n",
    "    var joinedData = labelData\n",
    "    for ((fg, i) <- fgs.zipWithIndex) {\n",
    "        val id = i + 2\n",
    "        val fg_id = s\"id_${id}\"\n",
    "        val fg_ts = s\"ts_${id}\"\n",
    "        joinedData = joinedData.hint(joinHint).join(\n",
    "            fg, (labelData(\"id\") === fg(fg_id)) && (labelData(\"ts\") >= fg(fg_ts))\n",
    "        ).unpersist()\n",
    "    }\n",
    "    // 2. Create window for partitioning and ordering the data\n",
    "    val orderByParams = \n",
    "        for (i <- 0 to fgs.length - 1)\n",
    "        yield desc(s\"ts_${i + 2}\")\n",
    "\n",
    "    val win = Window.partitionBy(\"id\", \"ts\").orderBy(orderByParams :_*)\n",
    "\n",
    "    // 3. Rank the rows of each partition\n",
    "    val rankedData = joinedData.withColumn(\"rank\", rank().over(win))\n",
    "\n",
    "    // 4. Take only the columns with rank == 1, for each partition\n",
    "    val filteredData = rankedData.filter(col(\"rank\") === 1)\n",
    "    \n",
    "    filteredData\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0643db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitJoin(\"BROADCAST\", fg1, fg2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val NO_RUNS = 10\n",
    "val HINTS = List(\"BROADCAST\", \"MERGE\")\n",
    "\n",
    "def experiment(labelData: DataFrame, fgs: DataFrame*) : Unit = {\n",
    "    for (hint <- HINTS) {\n",
    "        println(s\"Running with ${hint}\")\n",
    "        for (run <- 0 to NO_RUNS) {\n",
    "            spark.time(pitJoin(hint, labelData, fgs :_*).count)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93844331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('rank = 1)\n",
      "+- Project [id#0, ts#1, label#2, id_2#92, ts_2#93, f2#94, rank#10861]\n",
      "   +- Project [id#0, ts#1, label#2, id_2#92, ts_2#93, f2#94, rank#10861, rank#10861]\n",
      "      +- Window [rank(ts_2#93) windowspecdefinition(id#0, ts#1, ts_2#93 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#10861], [id#0, ts#1], [ts_2#93 DESC NULLS LAST]\n",
      "         +- Project [id#0, ts#1, label#2, id_2#92, ts_2#93, f2#94]\n",
      "            +- Join Inner, ((id#0 = id_2#92) AND (ts#1 >= ts_2#93))\n",
      "               :- ResolvedHint (strategy=broadcast)\n",
      "               :  +- Sort [ts#1 DESC NULLS LAST], true\n",
      "               :     +- Relation[id#0,ts#1,label#2] csv\n",
      "               +- Sort [ts_2#93 DESC NULLS LAST], true\n",
      "                  +- Relation[id_2#92,ts_2#93,f2#94] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, ts: int, label: string, id_2: int, ts_2: int, f2: string, rank: int\n",
      "Filter (rank#10861 = 1)\n",
      "+- Project [id#0, ts#1, label#2, id_2#92, ts_2#93, f2#94, rank#10861]\n",
      "   +- Project [id#0, ts#1, label#2, id_2#92, ts_2#93, f2#94, rank#10861, rank#10861]\n",
      "      +- Window [rank(ts_2#93) windowspecdefinition(id#0, ts#1, ts_2#93 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#10861], [id#0, ts#1], [ts_2#93 DESC NULLS LAST]\n",
      "         +- Project [id#0, ts#1, label#2, id_2#92, ts_2#93, f2#94]\n",
      "            +- Join Inner, ((id#0 = id_2#92) AND (ts#1 >= ts_2#93))\n",
      "               :- ResolvedHint (strategy=broadcast)\n",
      "               :  +- Sort [ts#1 DESC NULLS LAST], true\n",
      "               :     +- Relation[id#0,ts#1,label#2] csv\n",
      "               +- Sort [ts_2#93 DESC NULLS LAST], true\n",
      "                  +- Relation[id_2#92,ts_2#93,f2#94] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(rank#10861) AND (rank#10861 = 1))\n",
      "+- Window [rank(ts_2#93) windowspecdefinition(id#0, ts#1, ts_2#93 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#10861], [id#0, ts#1], [ts_2#93 DESC NULLS LAST]\n",
      "   +- Join Inner, ((id#0 = id_2#92) AND (ts#1 >= ts_2#93)), leftHint=(strategy=broadcast)\n",
      "      :- Filter (isnotnull(id#0) AND isnotnull(ts#1))\n",
      "      :  +- InMemoryRelation [id#0, ts#1, label#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :        +- *(1) Sort [ts#1 DESC NULLS LAST], true, 0\n",
      "      :           +- Exchange rangepartitioning(ts#1 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "      :              +- FileScan csv [id#0,ts#1,label#2] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "      +- Filter (isnotnull(id_2#92) AND isnotnull(ts_2#93))\n",
      "         +- InMemoryRelation [id_2#92, ts_2#93, f2#94], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- *(1) Sort [ts_2#93 DESC NULLS LAST], true, 0\n",
      "                  +- Exchange rangepartitioning(ts_2#93 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#41]\n",
      "                     +- FileScan csv [id_2#92,ts_2#93,f2#94] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id_2:int,ts_2:int,f2:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) Filter (isnotnull(rank#10861) AND (rank#10861 = 1))\n",
      "+- Window [rank(ts_2#93) windowspecdefinition(id#0, ts#1, ts_2#93 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#10861], [id#0, ts#1], [ts_2#93 DESC NULLS LAST]\n",
      "   +- *(3) Sort [id#0 ASC NULLS FIRST, ts#1 ASC NULLS FIRST, ts_2#93 DESC NULLS LAST], false, 0\n",
      "      +- Exchange hashpartitioning(id#0, ts#1, 200), ENSURE_REQUIREMENTS, [id=#4488]\n",
      "         +- *(2) BroadcastHashJoin [id#0], [id_2#92], Inner, BuildLeft, (ts#1 >= ts_2#93), false\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#4482]\n",
      "            :  +- *(1) Filter (isnotnull(id#0) AND isnotnull(ts#1))\n",
      "            :     +- InMemoryTableScan [id#0, ts#1, label#2], [isnotnull(id#0), isnotnull(ts#1)]\n",
      "            :           +- InMemoryRelation [id#0, ts#1, label#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            :                 +- *(1) Sort [ts#1 DESC NULLS LAST], true, 0\n",
      "            :                    +- Exchange rangepartitioning(ts#1 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "            :                       +- FileScan csv [id#0,ts#1,label#2] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "            +- *(2) Filter (isnotnull(id_2#92) AND isnotnull(ts_2#93))\n",
      "               +- InMemoryTableScan [id_2#92, ts_2#93, f2#94], [isnotnull(id_2#92), isnotnull(ts_2#93)]\n",
      "                     +- InMemoryRelation [id_2#92, ts_2#93, f2#94], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) Sort [ts_2#93 DESC NULLS LAST], true, 0\n",
      "                              +- Exchange rangepartitioning(ts_2#93 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#41]\n",
      "                                 +- FileScan csv [id_2#92,ts_2#93,f2#94] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id_2:int,ts_2:int,f2:string>\n",
      "\n",
      "res30: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "pitJoin(\"BROADCAST\", fg1, fg2).explain(extended=true)\n",
    "pitJoin(\"BROADCAST\", fg1, fg2).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eadab11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BROADCAST\n",
      "Time taken: 12893 ms\n",
      "Time taken: 8960 ms\n",
      "Time taken: 7949 ms\n",
      "Time taken: 8183 ms\n",
      "Time taken: 7470 ms\n",
      "Time taken: 6957 ms\n",
      "Time taken: 7147 ms\n",
      "Time taken: 6705 ms\n",
      "Time taken: 6560 ms\n",
      "Time taken: 7296 ms\n",
      "Time taken: 6240 ms\n",
      "Running with MERGE\n",
      "Time taken: 9180 ms\n",
      "Time taken: 7893 ms\n",
      "Time taken: 8463 ms\n",
      "Time taken: 7567 ms\n",
      "Time taken: 7843 ms\n",
      "Time taken: 7935 ms\n",
      "Time taken: 8201 ms\n",
      "Time taken: 7659 ms\n",
      "Time taken: 7557 ms\n",
      "Time taken: 7913 ms\n",
      "Time taken: 7766 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e2bc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BROADCAST\n",
      "Time taken: 18200 ms\n",
      "Time taken: 17016 ms\n",
      "Time taken: 17122 ms\n",
      "Time taken: 17084 ms\n",
      "Time taken: 17591 ms\n",
      "Time taken: 17793 ms\n",
      "Time taken: 17440 ms\n",
      "Time taken: 17626 ms\n",
      "Time taken: 17071 ms\n",
      "Time taken: 17242 ms\n",
      "Time taken: 17171 ms\n",
      "Running with MERGE\n",
      "Time taken: 15068 ms\n",
      "Time taken: 13814 ms\n",
      "Time taken: 13820 ms\n",
      "Time taken: 14225 ms\n",
      "Time taken: 13754 ms\n",
      "Time taken: 14151 ms\n",
      "Time taken: 13792 ms\n",
      "Time taken: 13953 ms\n",
      "Time taken: 13622 ms\n",
      "Time taken: 14056 ms\n",
      "Time taken: 13980 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557938c",
   "metadata": {},
   "source": [
    "# 2. Pre-sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f1ad2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sortedFg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res26: Long = 36779\n",
      "sortedFg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "res28: Long = 36779\n",
      "sortedFg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n",
      "res30: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "// Simulate pre-sorting of data\n",
    "// Data is already stored pre-sorted\n",
    "\n",
    "val sortedFg1 = spark.read.option(\"header\", true).schema(fg1_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\"\n",
    ").persist()\n",
    "sortedFg1.count()\n",
    "\n",
    "val sortedFg2 = spark.read.option(\"header\", true).schema(fg2_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").persist()\n",
    "sortedFg2.count()\n",
    "\n",
    "val sortedFg3 = spark.read.option(\"header\", true).schema(fg3_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").persist()\n",
    "sortedFg3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a02f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 4174 ms\n",
      "Time taken: 3861 ms\n",
      "Time taken: 3915 ms\n",
      "Time taken: 3854 ms\n",
      "Time taken: 3903 ms\n",
      "Time taken: 3908 ms\n",
      "Time taken: 3910 ms\n",
      "Time taken: 3864 ms\n",
      "Time taken: 3885 ms\n",
      "Time taken: 3845 ms\n",
      "Time taken: 3918 ms\n",
      "Running with MERGE\n",
      "Time taken: 4291 ms\n",
      "Time taken: 4187 ms\n",
      "Time taken: 4198 ms\n",
      "Time taken: 4179 ms\n",
      "Time taken: 4328 ms\n",
      "Time taken: 4394 ms\n",
      "Time taken: 4076 ms\n",
      "Time taken: 4235 ms\n",
      "Time taken: 4123 ms\n",
      "Time taken: 4300 ms\n",
      "Time taken: 4251 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(sortedFg1, sortedFg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa063660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 10444 ms\n",
      "Time taken: 10223 ms\n",
      "Time taken: 10047 ms\n",
      "Time taken: 10362 ms\n",
      "Time taken: 10170 ms\n",
      "Time taken: 10256 ms\n",
      "Time taken: 10098 ms\n",
      "Time taken: 10835 ms\n",
      "Time taken: 9975 ms\n",
      "Time taken: 10309 ms\n",
      "Time taken: 10117 ms\n",
      "Running with MERGE\n",
      "Time taken: 8209 ms\n",
      "Time taken: 8336 ms\n",
      "Time taken: 8580 ms\n",
      "Time taken: 8636 ms\n",
      "Time taken: 8490 ms\n",
      "Time taken: 8252 ms\n",
      "Time taken: 8108 ms\n",
      "Time taken: 8328 ms\n",
      "Time taken: 8402 ms\n",
      "Time taken: 8326 ms\n",
      "Time taken: 8290 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(sortedFg1, sortedFg2, sortedFg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70455d",
   "metadata": {},
   "source": [
    "# 3. Pre-partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ee786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id_2\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id_3\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9e51644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketedFg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "bucketedFg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "bucketedFg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val bucketedFg1 = spark.table(\"fg1_bucketed\").persist()\n",
    "val bucketedFg2 = spark.table(\"fg2_bucketed\").persist()\n",
    "val bucketedFg3 = spark.table(\"fg3_bucketed\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf00d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 9280 ms\n",
      "Time taken: 651 ms\n",
      "Time taken: 721 ms\n",
      "Time taken: 630 ms\n",
      "Time taken: 681 ms\n",
      "Time taken: 666 ms\n",
      "Time taken: 623 ms\n",
      "Time taken: 634 ms\n",
      "Time taken: 695 ms\n",
      "Time taken: 753 ms\n",
      "Time taken: 667 ms\n",
      "Running with MERGE\n",
      "Time taken: 1030 ms\n",
      "Time taken: 596 ms\n",
      "Time taken: 529 ms\n",
      "Time taken: 526 ms\n",
      "Time taken: 533 ms\n",
      "Time taken: 561 ms\n",
      "Time taken: 563 ms\n",
      "Time taken: 544 ms\n",
      "Time taken: 594 ms\n",
      "Time taken: 626 ms\n",
      "Time taken: 549 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(bucketedFg1, bucketedFg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b0cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 8131 ms\n",
      "Time taken: 4114 ms\n",
      "Time taken: 4154 ms\n",
      "Time taken: 4131 ms\n",
      "Time taken: 4090 ms\n",
      "Time taken: 4117 ms\n",
      "Time taken: 4428 ms\n",
      "Time taken: 4269 ms\n",
      "Time taken: 4274 ms\n",
      "Time taken: 4157 ms\n",
      "Time taken: 4149 ms\n",
      "Running with MERGE\n",
      "Time taken: 4566 ms\n",
      "Time taken: 4024 ms\n",
      "Time taken: 3929 ms\n",
      "Time taken: 3912 ms\n",
      "Time taken: 3828 ms\n",
      "Time taken: 3907 ms\n",
      "Time taken: 3894 ms\n",
      "Time taken: 3760 ms\n",
      "Time taken: 3894 ms\n",
      "Time taken: 3731 ms\n",
      "Time taken: 3920 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(bucketedFg1, bucketedFg2, bucketedFg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b57bf",
   "metadata": {},
   "source": [
    "# 4.Pre-partitioning and pre-sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3fe3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "sortedFg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "sortedFg2.write.mode(\"overwrite\").bucketBy(4, \"id_2\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "sortedFg3.write.mode(\"overwrite\").bucketBy(4, \"id_3\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7b2c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sortedBucketedFg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "sortedBucketedFg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "sortedBucketedFg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val sortedBucketedFg1 = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "val sortedBucketedFg2 = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "val sortedBucketedFg3 = spark.table(\"fg3_bucketed_sorted\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c340ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 1314 ms\n",
      "Time taken: 512 ms\n",
      "Time taken: 554 ms\n",
      "Time taken: 541 ms\n",
      "Time taken: 521 ms\n",
      "Time taken: 539 ms\n",
      "Time taken: 545 ms\n",
      "Time taken: 618 ms\n",
      "Time taken: 606 ms\n",
      "Time taken: 504 ms\n",
      "Time taken: 533 ms\n",
      "Running with MERGE\n",
      "Time taken: 492 ms\n",
      "Time taken: 541 ms\n",
      "Time taken: 540 ms\n",
      "Time taken: 548 ms\n",
      "Time taken: 534 ms\n",
      "Time taken: 607 ms\n",
      "Time taken: 517 ms\n",
      "Time taken: 505 ms\n",
      "Time taken: 568 ms\n",
      "Time taken: 528 ms\n",
      "Time taken: 569 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(sortedBucketedFg1, sortedBucketedFg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae9805fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 4231 ms\n",
      "Time taken: 3863 ms\n",
      "Time taken: 3765 ms\n",
      "Time taken: 3822 ms\n",
      "Time taken: 3759 ms\n",
      "Time taken: 3879 ms\n",
      "Time taken: 3837 ms\n",
      "Time taken: 3841 ms\n",
      "Time taken: 3872 ms\n",
      "Time taken: 3886 ms\n",
      "Time taken: 3831 ms\n",
      "Running with MERGE\n",
      "Time taken: 3930 ms\n",
      "Time taken: 3647 ms\n",
      "Time taken: 3841 ms\n",
      "Time taken: 3727 ms\n",
      "Time taken: 3859 ms\n",
      "Time taken: 3931 ms\n",
      "Time taken: 3844 ms\n",
      "Time taken: 3825 ms\n",
      "Time taken: 3976 ms\n",
      "Time taken: 3764 ms\n",
      "Time taken: 3851 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(sortedBucketedFg1, sortedBucketedFg2, sortedBucketedFg3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}