{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9cbf0e",
   "metadata": {},
   "source": [
    "# Optimizations for PIT-joins\n",
    "\n",
    "This notebook will consist of several optimizations for the existing join method. Stuff that will be looked into is the unoptimized PIT-join as well as optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8dd09",
   "metadata": {},
   "source": [
    "# 0. Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b17f1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>37</td><td>application_1642582607798_0036</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1642582607798_0036/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e03_1642582607798_0036_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
      "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.expressions.Window\n",
      "import io.hops.util.Hops\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import io.hops.util.Hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0714b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import io.github.ackuq.pit.{EarlyStopSortMerge, UnionAsOf}\n"
     ]
    }
   ],
   "source": [
    "import io.github.ackuq.pit.{EarlyStopSortMerge, UnionAsOf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67dc9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(ts,IntegerType,true), StructField(label,StringType,true))\n",
      "fg2_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(ts,IntegerType,true), StructField(f2,StringType,true))\n",
      "fg3_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(ts,IntegerType,true), StructField(f3,StringType,true))\n"
     ]
    }
   ],
   "source": [
    "val fg1_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"ts\", IntegerType, true),\n",
    "  StructField(\"label\", StringType, true)    \n",
    "))\n",
    "\n",
    "val fg2_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"ts\", IntegerType, true),\n",
    "  StructField(\"f2\", StringType, true)    \n",
    "))\n",
    "\n",
    "val fg3_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"ts\", IntegerType, true),\n",
    "  StructField(\"f3\", StringType, true)    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf3f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@55aea356\n"
     ]
    }
   ],
   "source": [
    "val spark: SparkSession = SparkSession.builder().master(\"local\").appName(\"PIT Optimizations Scala\").config(\"spark.sql.adaptive.enabled\", true).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffd0e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use \" + Hops.getProjectName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e417bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopSortMerge.init(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc659c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data2: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data3: Seq[org.apache.spark.sql.Row] = List([1,1,f3-1-1], [2,2,f3-2-2], [1,6,f3-1-6], [2,8,f3-2-8], [1,10,f3-1-10])\n"
     ]
    }
   ],
   "source": [
    "val data1 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data2 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data3 = Seq(\n",
    "    Row(1, 1, \"f3-1-1\"),\n",
    "    Row(2, 2, \"f3-2-2\"),\n",
    "    Row(1, 6, \"f3-1-6\"),\n",
    "    Row(2, 8, \"f3-2-8\"),\n",
    "    Row(1, 10, \"f3-1-10\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2b7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val fg1 = spark.createDataFrame(spark.sparkContext.parallelize(data1), schema=fg1_schema) \n",
    "val fg2 = spark.createDataFrame(spark.sparkContext.parallelize(data2), schema=fg2_schema) \n",
    "val fg3 = spark.createDataFrame(spark.sparkContext.parallelize(data3), schema=fg3_schema) \n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3507b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: String = hdfs:///Projects/demo_fs_meb10000/Jupyter/PIT-joins/example-data\n",
      "fg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res11: Long = 36779\n",
      "fg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res13: Long = 36779\n",
      "fg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res15: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val DATA_PATH = \"hdfs:///Projects/\" + Hops.getProjectName + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "\n",
    "val fg1 = spark.read.option(\"header\", true).schema(fg1_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg1.count()\n",
    "\n",
    "val fg2 = spark.read.option(\"header\", true).schema(fg2_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg2.count()\n",
    "\n",
    "val fg3 = spark.read.option(\"header\", true).schema(fg3_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d3d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: org.apache.spark.sql.DataFrame = [id: bigint, ts: bigint ... 1 more field]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4767027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: org.apache.spark.sql.DataFrame = [id: bigint, ts: bigint ... 1 more field]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ca8cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: org.apache.spark.sql.DataFrame = [id: bigint, ts: bigint ... 1 more field]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae9a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: org.apache.spark.sql.DataFrame = [id: bigint, ts: bigint ... 1 more field]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1afdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1400|   f1|\n",
      "|98163|1400|   f1|\n",
      "|98164|1400|   f1|\n",
      "|98165|1400|   f1|\n",
      "|98166|1400|   f1|\n",
      "|98167|1400|   f1|\n",
      "|98168|1400|   f1|\n",
      "|98169|1400|   f1|\n",
      "|98170|1400|   f1|\n",
      "|98171|1400|   f1|\n",
      "|98172|1400|   f1|\n",
      "|98173|1400|   f1|\n",
      "|98174|1400|   f1|\n",
      "|98175|1400|   f1|\n",
      "|98176|1400|   f1|\n",
      "|98177|1400|   f1|\n",
      "|98178|1400|   f1|\n",
      "|98179|1400|   f1|\n",
      "|98180|1400|   f1|\n",
      "|98181|1400|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fg1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b458c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg1.createOrReplaceTempView(\"left\")\n",
    "fg2.createOrReplaceTempView(\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fff3f",
   "metadata": {},
   "source": [
    "# 1. Union PIT Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94071c31",
   "metadata": {},
   "source": [
    "## 1.1. Sorted on timestamps (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8824aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unionPitJoin: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)org.apache.spark.sql.DataFrame\n",
      "+-----+----+-----+------+------+------+------+\n",
      "|   id|  ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+-----+----+-----+------+------+------+------+\n",
      "|98164|1020|   f1|  1020|    f2|  1020|    f2|\n",
      "|98164|1040|   f1|  1040|    f2|  1040|    f2|\n",
      "|98164|1060|   f1|  1060|    f2|  1060|    f2|\n",
      "|98164|1080|   f1|  1080|    f2|  1080|    f2|\n",
      "|98164|1100|   f1|  1100|    f2|  1100|    f2|\n",
      "|98164|1120|   f1|  1120|    f2|  1120|    f2|\n",
      "|98164|1140|   f1|  1140|    f2|  1140|    f2|\n",
      "|98164|1160|   f1|  1160|    f2|  1160|    f2|\n",
      "|98164|1180|   f1|  1180|    f2|  1180|    f2|\n",
      "|98164|1200|   f1|  1200|    f2|  1200|    f2|\n",
      "|98164|1220|   f1|  1220|    f2|  1220|    f2|\n",
      "|98164|1240|   f1|  1240|    f2|  1240|    f2|\n",
      "|98164|1260|   f1|  1260|    f2|  1260|    f2|\n",
      "|98164|1280|   f1|  1280|    f2|  1280|    f2|\n",
      "|98164|1300|   f1|  1300|    f2|  1300|    f2|\n",
      "|98164|1320|   f1|  1320|    f2|  1320|    f2|\n",
      "|98164|1340|   f1|  1340|    f2|  1340|    f2|\n",
      "|98164|1360|   f1|  1360|    f2|  1360|    f2|\n",
      "|98164|1380|   f1|  1380|    f2|  1380|    f2|\n",
      "|98164|1400|   f1|  1400|    f2|  1400|    f2|\n",
      "+-----+----+-----+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def unionPitJoin(labelData: DataFrame, fgs: DataFrame*) : DataFrame = {\n",
    "    var joinedData = labelData\n",
    "    for ((fg, i) <- fgs.zipWithIndex) {\n",
    "        val num = i + 2\n",
    "        joinedData = UnionAsOf.join(\n",
    "            joinedData,\n",
    "            fg,\n",
    "            rightPrefix = s\"fg${num}_\",\n",
    "            partitionCols = Seq(\"id\")\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    joinedData\n",
    "}\n",
    "\n",
    "unionPitJoin(fg1, fg2, fg3).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c62d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "+- Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "         +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926, fg2_f2#962, fg2_f2#962]\n",
      "            +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "               +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "                  +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#926, df_combined_ts#935]\n",
      "                     +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925, fg2_ts#953, fg2_ts#953]\n",
      "                        +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                           +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "                              +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935]\n",
      "                                 +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1, id#944, id#944]\n",
      "                                    +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                                       +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1]\n",
      "                                          +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true\n",
      "                                             +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, coalesce(ts#2, fg2_ts#925) AS df_combined_ts#935]\n",
      "                                                +- Union false, false\n",
      "                                                   :- Project [id#1, ts#2, label#3, df_index#889, cast(fg2_ts#894 as int) AS fg2_ts#925, cast(fg2_f2#900 as string) AS fg2_f2#926]\n",
      "                                                   :  +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#894, null AS fg2_f2#900]\n",
      "                                                   :     +- Project [id#1, ts#2, label#3, df_index#889, null AS fg2_ts#894]\n",
      "                                                   :        +- Project [id#1, ts#2, label#3, 1 AS df_index#889]\n",
      "                                                   :           +- Sort [ts#2 DESC NULLS LAST], true\n",
      "                                                   :              +- Relation[id#1,ts#2,label#3] csv\n",
      "                                                   +- Project [id#108, cast(ts#912 as int) AS ts#927, cast(label#918 as string) AS label#928, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                      +- Project [id#108, ts#912, label#918, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                         +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, ts#912, null AS label#918]\n",
      "                                                            +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, null AS ts#912]\n",
      "                                                               +- Project [id#108, fg2_ts#884, fg2_f2#885, 0 AS df_index#907]\n",
      "                                                                  +- Project [id#108, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885]\n",
      "                                                                     +- Sort [ts#109 DESC NULLS LAST], true\n",
      "                                                                        +- Relation[id#108,ts#109,f2#110] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, ts: int, label: string, fg2_ts: int, fg2_f2: string\n",
      "Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "+- Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "         +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926, fg2_f2#962, fg2_f2#962]\n",
      "            +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "               +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "                  +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#926, df_combined_ts#935]\n",
      "                     +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925, fg2_ts#953, fg2_ts#953]\n",
      "                        +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                           +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "                              +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935]\n",
      "                                 +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1, id#944, id#944]\n",
      "                                    +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                                       +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1]\n",
      "                                          +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true\n",
      "                                             +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, coalesce(ts#2, fg2_ts#925) AS df_combined_ts#935]\n",
      "                                                +- Union false, false\n",
      "                                                   :- Project [id#1, ts#2, label#3, df_index#889, cast(fg2_ts#894 as int) AS fg2_ts#925, cast(fg2_f2#900 as string) AS fg2_f2#926]\n",
      "                                                   :  +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#894, null AS fg2_f2#900]\n",
      "                                                   :     +- Project [id#1, ts#2, label#3, df_index#889, null AS fg2_ts#894]\n",
      "                                                   :        +- Project [id#1, ts#2, label#3, 1 AS df_index#889]\n",
      "                                                   :           +- Sort [ts#2 DESC NULLS LAST], true\n",
      "                                                   :              +- Relation[id#1,ts#2,label#3] csv\n",
      "                                                   +- Project [id#108, cast(ts#912 as int) AS ts#927, cast(label#918 as string) AS label#928, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                      +- Project [id#108, ts#912, label#918, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                         +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, ts#912, null AS label#918]\n",
      "                                                            +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, null AS ts#912]\n",
      "                                                               +- Project [id#108, fg2_ts#884, fg2_f2#885, 0 AS df_index#907]\n",
      "                                                                  +- Project [id#108, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885]\n",
      "                                                                     +- Sort [ts#109 DESC NULLS LAST], true\n",
      "                                                                        +- Relation[id#108,ts#109,f2#110] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "+- Filter isnotnull(ts#2)\n",
      "   +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "      +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "         +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "            +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "               +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                  +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1]\n",
      "                     +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true\n",
      "                        +- Union false, false\n",
      "                           :- Project [id#1, ts#2, label#3, 1 AS df_index#889, null AS fg2_ts#925, null AS fg2_f2#926, ts#2 AS df_combined_ts#935]\n",
      "                           :  +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           :        +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "                           :           +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "                           :              +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "                           +- Project [id#108, null AS ts#927, null AS label#928, 0 AS df_index#907, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885, ts#109 AS df_combined_ts#1041]\n",
      "                              +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                    +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                                       +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                                          +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "         +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "            +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "               +- Sort [id#944 ASC NULLS FIRST, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#944, 200), ENSURE_REQUIREMENTS, [id=#1232]\n",
      "                     +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "                        +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                           +- Sort [id#1 ASC NULLS FIRST, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(id#1, 200), ENSURE_REQUIREMENTS, [id=#1227]\n",
      "                                 +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true, 0\n",
      "                                    +- Exchange rangepartitioning(df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#1224]\n",
      "                                       +- Union\n",
      "                                          :- Project [id#1, ts#2, label#3, 1 AS df_index#889, null AS fg2_ts#925, null AS fg2_f2#926, ts#2 AS df_combined_ts#935]\n",
      "                                          :  +- InMemoryTableScan [id#1, label#3, ts#2]\n",
      "                                          :        +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                          :              +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "                                          :                 +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "                                          :                    +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "                                          +- Project [id#108, null AS ts#927, null AS label#928, 0 AS df_index#907, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885, ts#109 AS df_combined_ts#1041]\n",
      "                                             +- InMemoryTableScan [f2#110, id#108, ts#109]\n",
      "                                                   +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                         +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                                                            +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                                                               +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2).explain(extended=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb80edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------+------+\n",
      "|   id|  ts|label|fg2_ts|fg2_f2|\n",
      "+-----+----+-----+------+------+\n",
      "|98164|1020|   f1|  1020|    f2|\n",
      "|98164|1040|   f1|  1040|    f2|\n",
      "|98164|1060|   f1|  1060|    f2|\n",
      "|98164|1080|   f1|  1080|    f2|\n",
      "|98164|1100|   f1|  1100|    f2|\n",
      "|98164|1120|   f1|  1120|    f2|\n",
      "|98164|1140|   f1|  1140|    f2|\n",
      "|98164|1160|   f1|  1160|    f2|\n",
      "|98164|1180|   f1|  1180|    f2|\n",
      "|98164|1200|   f1|  1200|    f2|\n",
      "|98164|1220|   f1|  1220|    f2|\n",
      "|98164|1240|   f1|  1240|    f2|\n",
      "|98164|1260|   f1|  1260|    f2|\n",
      "|98164|1280|   f1|  1280|    f2|\n",
      "|98164|1300|   f1|  1300|    f2|\n",
      "|98164|1320|   f1|  1320|    f2|\n",
      "|98164|1340|   f1|  1340|    f2|\n",
      "|98164|1360|   f1|  1360|    f2|\n",
      "|98164|1380|   f1|  1380|    f2|\n",
      "|98164|1400|   f1|  1400|    f2|\n",
      "+-----+----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2715fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: org.apache.spark.sql.DataFrame = [id: int, ts: int ... 3 more fields]\n",
      "res23: Long = 36779\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1401, ts#2, label#3, fg2_ts#1410, fg2_f2#1419]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Window [last(fg2_f2#1383, true) windowspecdefinition(id#1401, df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#1419], [id#1401], [df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST]\n",
      "         +- Project [id#1401, ts#2, label#3, df_index#1346, fg2_ts#1410, df_combined_ts#1392, fg2_f2#1383]\n",
      "            +- Window [last(fg2_ts#1382, true) windowspecdefinition(id#1401, df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#1410], [id#1401], [df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST]\n",
      "               +- Sort [id#1401 ASC NULLS FIRST, df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#1401, 200), ENSURE_REQUIREMENTS, [id=#1700]\n",
      "                     +- Project [id#1401, ts#2, label#3, df_index#1346, fg2_f2#1383, df_combined_ts#1392, fg2_ts#1382]\n",
      "                        +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#1401], [id#1], [df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST]\n",
      "                           +- Sort [id#1 ASC NULLS FIRST, df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(id#1, 200), ENSURE_REQUIREMENTS, [id=#1695]\n",
      "                                 +- Sort [df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST], true, 0\n",
      "                                    +- Exchange rangepartitioning(df_combined_ts#1392 ASC NULLS FIRST, df_index#1346 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#1692]\n",
      "                                       +- Union\n",
      "                                          :- Project [id#1, ts#2, label#3, 1 AS df_index#1346, null AS fg2_ts#1382, null AS fg2_f2#1383, ts#2 AS df_combined_ts#1392]\n",
      "                                          :  +- InMemoryTableScan [id#1, label#3, ts#2]\n",
      "                                          :        +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                          :              +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "                                          :                 +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "                                          :                    +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "                                          +- Project [id#108, null AS ts#1384, null AS label#1385, 0 AS df_index#1364, ts#109 AS fg2_ts#1341, f2#110 AS fg2_f2#1342, ts#109 AS df_combined_ts#1614]\n",
      "                                             +- InMemoryTableScan [f2#110, id#108, ts#109]\n",
      "                                                   +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                         +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                                                            +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                                                               +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val query = unionPitJoin(fg1, fg2)\n",
    "query.count\n",
    "query.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b314c739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_RUNS: Int = 10\n",
      "experimentUnion: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)Unit\n"
     ]
    }
   ],
   "source": [
    "val NO_RUNS = 10\n",
    "\n",
    "def experimentUnion(labelData: DataFrame, fgs: DataFrame*) : Unit = {\n",
    "    for (run <- 1 to NO_RUNS) {\n",
    "        spark.time(unionPitJoin(labelData, fgs :_*).show(0))\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a6767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2).show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa14866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2, fg3).show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc2e56e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2424 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2318 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2162 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2305 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2137 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2303 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2321 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2618 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2375 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2237 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentUnion(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f75c37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 14541 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 14609 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 14026 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 14315 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 14010 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 14242 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13229 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13756 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13017 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13410 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3ea2e",
   "metadata": {},
   "source": [
    "## 1.2. Sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea04f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1Sorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res34: Long = 36779\n",
      "fg2Sorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res35: Long = 36779\n",
      "fg3Sorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res36: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1Sorted = fg1.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg1Sorted.count\n",
    "val fg2Sorted = fg2.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg2Sorted.count\n",
    "val fg3Sorted = fg3.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg3Sorted.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e772c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12698 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12281 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12103 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12481 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12048 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11798 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11965 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12101 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12007 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12107 ms\n"
     ]
    }
   ],
   "source": [
    "// One feature group\n",
    "\n",
    "experimentUnion(fg1Sorted, fg2Sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9684d54",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e58fa02e8cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'// Two feature groups\\n\\nexperimentUnion(fg1Sorted, fg2Sorted, fg3Sorted)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_final\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmimetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_session_on_spark_statement_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1Sorted, fg2Sorted, fg3Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb0ba9",
   "metadata": {},
   "source": [
    "## 1.3 Bucketed sorted on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fg1Bucketed = spark.table(\"fg1_bucketed\").persist()\n",
    "fg1Bucketed.count\n",
    "val fg2Bucketed = spark.table(\"fg2_bucketed\").persist()\n",
    "fg2Bucketed.count\n",
    "val fg3Bucketed = spark.table(\"fg3_bucketed\").persist()\n",
    "fg3Bucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00470cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentUnion(fg1Bucketed, fg2Bucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1Bucketed, fg2Bucketed, fg3Bucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bc8e3",
   "metadata": {},
   "source": [
    "## 1.4. Bucketed, sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "fg2Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "fg3Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fg1SortedBucketed = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "fg1SortedBucketed.count\n",
    "val fg2SortedBucketed = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "fg2SortedBucketed.count\n",
    "val fg3SortedBucketed = spark.table(\"fg3_bucketed_sorted\").persist()\n",
    "fg3SortedBucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850cee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentUnion(fg1SortedBucketed, fg2SortedBucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172559f0",
   "metadata": {},
   "source": [
    "# 2. Early stop sort merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0d5c2",
   "metadata": {},
   "source": [
    "## 2.1 Sorted on timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb4dccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earlyStopSortMerge: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)org.apache.spark.sql.DataFrame\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- PITJoin [ts#2], [ts#109], [id#1], [id#108]\n",
      "   :- Sort [id#1 DESC NULLS LAST, ts#2 DESC NULLS LAST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#1, 200), ENSURE_REQUIREMENTS, [id=#31054]\n",
      "   :     +- Filter isnotnull(id#1)\n",
      "   :        +- InMemoryTableScan [id#1, ts#2, label#3], [isnotnull(id#1)]\n",
      "   :              +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                    +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "   :                       +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "   :                          +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "   +- Sort [id#108 DESC NULLS LAST, ts#109 DESC NULLS LAST], false, 0\n",
      "      +- Exchange hashpartitioning(id#108, 200), ENSURE_REQUIREMENTS, [id=#31055]\n",
      "         +- Filter isnotnull(id#108)\n",
      "            +- InMemoryTableScan [id#108, ts#109, f2#110], [isnotnull(id#108)]\n",
      "                  +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                           +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                              +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def earlyStopSortMerge(labelData: DataFrame, fgs: DataFrame*): DataFrame = {\n",
    "    var joinedData = labelData\n",
    "    for (fg <- fgs) {\n",
    "        joinedData = joinedData.join(\n",
    "            fg,\n",
    "            EarlyStopSortMerge.pit(labelData(\"ts\"), fg(\"ts\")) && labelData(\"id\") === fg(\"id\")\n",
    "        )\n",
    "    }\n",
    "    joinedData\n",
    "}\n",
    "\n",
    "earlyStopSortMerge(fg1, fg2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e67378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------+----+---+\n",
      "|    id|  ts|label|    id|  ts| f2|\n",
      "+------+----+-----+------+----+---+\n",
      "|100000|1400|   f1|100000|1400| f2|\n",
      "|100000|1380|   f1|100000|1380| f2|\n",
      "|100000|1360|   f1|100000|1360| f2|\n",
      "|100000|1340|   f1|100000|1340| f2|\n",
      "|100000|1320|   f1|100000|1320| f2|\n",
      "|100000|1300|   f1|100000|1300| f2|\n",
      "|100000|1280|   f1|100000|1280| f2|\n",
      "|100000|1260|   f1|100000|1260| f2|\n",
      "|100000|1240|   f1|100000|1240| f2|\n",
      "|100000|1220|   f1|100000|1220| f2|\n",
      "|100000|1200|   f1|100000|1200| f2|\n",
      "|100000|1180|   f1|100000|1180| f2|\n",
      "|100000|1160|   f1|100000|1160| f2|\n",
      "|100000|1140|   f1|100000|1140| f2|\n",
      "|100000|1120|   f1|100000|1120| f2|\n",
      "|100000|1100|   f1|100000|1100| f2|\n",
      "|100000|1080|   f1|100000|1080| f2|\n",
      "|100000|1060|   f1|100000|1060| f2|\n",
      "|100000|1040|   f1|100000|1040| f2|\n",
      "|100000|1020|   f1|100000|1020| f2|\n",
      "+------+----+-----+------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earlyStopSortMerge(fg1, fg2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69c1acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3690 ms\n",
      "res61: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "spark.time(earlyStopSortMerge(fg1, fg2).count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f26dc3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_RUNS: Int = 10\n",
      "experimentEarlyStop: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)Unit\n"
     ]
    }
   ],
   "source": [
    "val NO_RUNS = 10\n",
    "\n",
    "def experimentEarlyStop(labelData: DataFrame, fgs: DataFrame*) : Unit = {\n",
    "    for (run <- 0 to NO_RUNS) {\n",
    "        spark.time(earlyStopSortMerge(labelData, fgs :_*).count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// One feature group\n",
    "experimentEarlyStop(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817616b3",
   "metadata": {},
   "source": [
    "## 2.2. Sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fg1Sorted = fg1.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg1Sorted.count\n",
    "val fg2Sorted = fg2.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg2Sorted.count\n",
    "val fg3Sorted = fg3.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg3Sorted.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49454b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "// One feature group\n",
    "\n",
    "experimentEarlyStop(fg1Sorted, fg2Sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1Sorted, fg2Sorted, fg3Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb727e0",
   "metadata": {},
   "source": [
    "## 2.3. Bucketed, sorted on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c60681",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fg1Bucketed = spark.table(\"fg1_bucketed\").persist()\n",
    "fg1Bucketed.count\n",
    "val fg2Bucketed = spark.table(\"fg2_bucketed\").persist()\n",
    "fg2Bucketed.count\n",
    "val fg3Bucketed = spark.table(\"fg3_bucketed\").persist()\n",
    "fg3Bucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentEarlyStop(fg1Bucketed, fg2Bucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1Bucketed, fg2Bucketed, fg3Bucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c709c",
   "metadata": {},
   "source": [
    "## 2.4. Bucketed, sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7cbb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "fg2Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "fg3Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aa3bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res63: Long = 36779\n",
      "fg2SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res64: Long = 36779\n",
      "fg3SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res65: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1SortedBucketed = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "fg1SortedBucketed.count\n",
    "val fg2SortedBucketed = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "fg2SortedBucketed.count\n",
    "val fg3SortedBucketed = spark.table(\"fg3_bucketed_sorted\").persist()\n",
    "fg3SortedBucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56060e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 386 ms\n",
      "Time taken: 263 ms\n",
      "Time taken: 254 ms\n",
      "Time taken: 265 ms\n",
      "Time taken: 242 ms\n",
      "Time taken: 236 ms\n",
      "Time taken: 254 ms\n",
      "Time taken: 269 ms\n",
      "Time taken: 237 ms\n",
      "Time taken: 210 ms\n",
      "Time taken: 229 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentEarlyStop(fg1SortedBucketed, fg2SortedBucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9801c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1241 ms\n",
      "Time taken: 303 ms\n",
      "Time taken: 370 ms\n",
      "Time taken: 460 ms\n",
      "Time taken: 444 ms\n",
      "Time taken: 330 ms\n",
      "Time taken: 296 ms\n",
      "Time taken: 322 ms\n",
      "Time taken: 312 ms\n",
      "Time taken: 314 ms\n",
      "Time taken: 334 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adaeb3b",
   "metadata": {},
   "source": [
    "## 2.5 Pre-partitioned, sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "478d52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1Sorted.write.mode(\"overwrite\").partitionBy(\"id\").saveAsTable(\"fg1_partitioned_sorted\")\n",
    "fg2Sorted.write.mode(\"overwrite\").partitionBy(\"id\").saveAsTable(\"fg2_partitioned_sorted\")\n",
    "fg3Sorted.write.mode(\"overwrite\").partitionBy(\"id\").saveAsTable(\"fg3_partitioned_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f9762ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1SortedPartitioned: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ts: int, label: string ... 1 more field]\n",
      "res76: Long = 36779\n",
      "fg2SortedPartitioned: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ts: int, f2: string ... 1 more field]\n",
      "res77: Long = 36779\n",
      "fg3SortedPartitioned: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ts: int, f3: string ... 1 more field]\n",
      "res78: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1SortedPartitioned = spark.table(\"fg1_partitioned_sorted\").persist()\n",
    "fg1SortedPartitioned.count\n",
    "val fg2SortedPartitioned = spark.table(\"fg2_partitioned_sorted\").persist()\n",
    "fg2SortedPartitioned.count\n",
    "val fg3SortedPartitioned = spark.table(\"fg3_partitioned_sorted\").persist()\n",
    "fg3SortedPartitioned.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "220f5aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3883 ms\n",
      "Time taken: 3696 ms\n",
      "Time taken: 3538 ms\n",
      "Time taken: 3482 ms\n",
      "Time taken: 3958 ms\n",
      "Time taken: 3608 ms\n",
      "Time taken: 3436 ms\n",
      "Time taken: 3628 ms\n",
      "Time taken: 3549 ms\n",
      "Time taken: 3425 ms\n",
      "Time taken: 3592 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1SortedPartitioned, fg2SortedPartitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5573 ms\n",
      "Time taken: 5241 ms\n",
      "Time taken: 5370 ms\n",
      "Time taken: 5392 ms\n",
      "Time taken: 5251 ms\n",
      "Time taken: 5283 ms\n",
      "Time taken: 5480 ms\n",
      "Time taken: 5116 ms\n",
      "Time taken: 5528 ms\n",
      "Time taken: 5224 ms\n",
      "Time taken: 5345 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1SortedPartitioned, fg2SortedPartitioned, fg3SortedPartitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06f4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}