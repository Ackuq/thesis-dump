{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27cba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>43</td><td>application_1646303173729_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1646303173729_0002/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e04_1646303173729_0002_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import hsfs\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c990b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "connection = hsfs.connection()\n",
    "fs = connection.get_feature_store()\n",
    "\n",
    "# Storage connector to s3\n",
    "sc = fs.get_storage_connector(\"experiment-s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e924a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.prepare_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1404b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_directory = \"s3a://\" + sc.bucket + \"/axel_experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f90935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Data Generation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12558813",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1337)\n",
    "SECONDS_IN_DAY = 60 * 60 * 24\n",
    "SECONDS_IN_MONTH = SECONDS_IN_DAY * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54945e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"ts\", IntegerType(), False),\n",
    "        StructField(\"label\", StringType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "RIGHT_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"ts\", IntegerType(), False),\n",
    "        StructField(\"value\", StringType(), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a950de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DistributionConfiguration:\n",
    "    mean: float\n",
    "    sd: float\n",
    "\n",
    "\n",
    "class TimestampDistribution(Enum):\n",
    "    Normal = \"normal\"\n",
    "    Uniform = \"uniform\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a802969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_value(value, min_v, max_v):\n",
    "    return min(max(min_v, value), max_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03591325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_left_table(unique_ids: int, max_timestamp: int) -> DataFrame:\n",
    "    # Create RDD of all initial RDDs\n",
    "    id_rdd = spark.sparkContext.parallelize([id for id in range(1, unique_ids + 1)])\n",
    "    left_rdd = id_rdd.map(lambda id : (\n",
    "        id, \n",
    "        rng.integers(low=0, high=max_timestamp).item(),\n",
    "        \"{}\".format(id),\n",
    "    ))\n",
    "    return left_rdd.toDF(LEFT_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fa039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_right_table(\n",
    "    unique_ids: int,\n",
    "    max_timestamp: int,\n",
    "    events_per_id_confs: List[DistributionConfiguration],\n",
    "    timestamp_distributions: List[TimestampDistribution],\n",
    "):\n",
    "    id_rdd = spark.sparkContext.parallelize([id for id in range(1, unique_ids + 1)])\n",
    "    def mapping_func(id):\n",
    "        # Pick a event generation conf at random\n",
    "        events_per_id_conf_idx = rng.integers(\n",
    "            low=0, high=len(events_per_id_confs), size=1\n",
    "        )[0]\n",
    "        events_per_id_conf = events_per_id_confs[events_per_id_conf_idx]\n",
    "        events = max(rng.normal(loc=events_per_id_conf.mean, scale=events_per_id_conf.sd, size=1)[0].astype(int), 0)  # type: ignore\n",
    "\n",
    "        # Pick a timestamp distribution configuration\n",
    "        timestamp_distribution_idx = rng.integers(\n",
    "            low=0, high=len(timestamp_distributions), size=1\n",
    "        )[0]\n",
    "        timestamp_distribution = timestamp_distributions[timestamp_distribution_idx]\n",
    "\n",
    "        if timestamp_distribution is TimestampDistribution.Uniform:\n",
    "            timestamps = rng.integers(low=0, high=max_timestamp, size=events).tolist()\n",
    "        elif timestamp_distribution is TimestampDistribution.Normal:\n",
    "            mean = rng.uniform(low=0, high=max_timestamp, size=1)[0]\n",
    "            sd = rng.uniform(low=SECONDS_IN_DAY * 5, high=SECONDS_IN_DAY * 15)\n",
    "            timestamps = rng.normal(mean, sd, events).astype(int).tolist()  # type: ignore\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid timestamp distribution: {}\".format(timestamp_distribution)\n",
    "            )\n",
    "        return [\n",
    "            (id, limit_value(timestamp, 0, max_timestamp  - 1), \"{}\".format(id))\n",
    "            for timestamp in timestamps\n",
    "        ]\n",
    "\n",
    "    right_rdd = id_rdd.flatMap(mapping_func)\n",
    "    return right_rdd.toDF(RIGHT_SCHEMA)\n",
    "\n",
    "\"\"\"\n",
    "generate_right_table(1_000_000, SECONDS_IN_MONTH * 12, [\n",
    "    DistributionConfiguration(20, 2),\n",
    "    DistributionConfiguration(80, 8),\n",
    "], [TimestampDistribution.Uniform, TimestmapDistribution.Normal]).count()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c516f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(ids, max_ts, left: DataFrame, right: DataFrame):\n",
    "    directory = s3_directory + \"/raw/{}-{}\".format(ids, max_ts)\n",
    "    left.write.parquet(directory + \"/left.parquet\", mode=\"overwrite\")\n",
    "    # left.to_csv(directory + \"/debug_left.csv\")\n",
    "    right.write.parquet(directory + \"/right.parquet\", mode=\"overwrite\")\n",
    "    # right.to_csv(directory + \"/debug_right.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf905d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_IDS = [10_000, 100_000, 1_000_000, 10_000_000]\n",
    "INTERVALS = [\n",
    "    (SECONDS_IN_MONTH * 12, \"1_year\"),\n",
    "]\n",
    "FEATURE_UPDATES = [\n",
    "    DistributionConfiguration(20, 2),\n",
    "    DistributionConfiguration(80, 8),\n",
    "]\n",
    "NO_FEATURES = [1, 2]\n",
    "DISTRIBUTIONS = [TimestampDistribution.Normal, TimestampDistribution.Uniform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72726074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all():\n",
    "    progress = tqdm(total=sum(UNIQUE_IDS))\n",
    "    for interval in INTERVALS:\n",
    "        for unique_ids in UNIQUE_IDS:\n",
    "            left = generate_left_table(unique_ids, interval[0])\n",
    "            right = generate_right_table(\n",
    "                unique_ids, interval[0], FEATURE_UPDATES, DISTRIBUTIONS\n",
    "            )\n",
    "            save(\n",
    "                unique_ids,\n",
    "                interval[1],\n",
    "                left,\n",
    "                right,\n",
    "            )\n",
    "            progress.update(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c79042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o257.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Serialized task 10:0 was 510075758 bytes, which exceeds max allowed: spark.rpc.message.maxSize (134217728 bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values.\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\t... 33 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 9, in generate_all\n",
      "  File \"<stdin>\", line 5, in save\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1249, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o257.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Serialized task 10:0 was 510075758 bytes, which exceeds max allowed: spark.rpc.message.maxSize (134217728 bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values.\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\t... 33 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108ba08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}