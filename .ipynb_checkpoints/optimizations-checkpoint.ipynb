{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a8b0c1",
   "metadata": {},
   "source": [
    "# Optimizations for PIT-joins\n",
    "\n",
    "This notebook will consist of several optimizations for the existing join method. Stuff that will be looked into is the unoptimized PIT-join as well as optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db660d",
   "metadata": {},
   "source": [
    "## 0. Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90c1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>11</td><td>application_1642582607798_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1642582607798_0010/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e03_1642582607798_0010_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row, SparkSession, Window, SQLContext\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col\n",
    "from pyspark.sql import functions as F\n",
    "from hops import hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[1, 5, \"1x\"],\n",
    "         [1, 7, \"1y\"],\n",
    "         [1, 4, \"1z\"],\n",
    "         [2, 6, \"2x\"],\n",
    "         [2, 8, \"2y\"]]\n",
    "\n",
    "data2 = [[1, 5, \"1x\"],\n",
    "         [1, 7, \"1y\"],\n",
    "         [1, 4, \"1z\"],\n",
    "         [2, 6, \"2x\"],\n",
    "         [2, 8, \"2y\"]]\n",
    "\n",
    "data3 = [[1, 10, \"f3-1-10\"],\n",
    "         [1, 1, \"f3-1-1\"],\n",
    "         [1, 6, \"f3-1-6\"],\n",
    "         [2, 2, \"f3-2-2\"],\n",
    "         [2, 8, \"f3-2-8\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0fbdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Three Way PIT\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "sql_context = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f39821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg1_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), False),\n",
    "  StructField(\"ts\", IntegerType(), False),\n",
    "  StructField(\"label\", StringType(), False)    \n",
    "])\n",
    "\n",
    "fg2_schema = StructType([\n",
    "  StructField(\"id_2\", IntegerType(), False),\n",
    "  StructField(\"ts_2\", IntegerType(), False),\n",
    "  StructField(\"f2\", StringType(), False)\n",
    "])\n",
    "\n",
    "fg3_schema = StructType([\n",
    "  StructField(\"id_3\", IntegerType(), False),\n",
    "  StructField(\"ts_3\", IntegerType(), False),\n",
    "  StructField(\"f3\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73ae5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fg1 = spark.createDataFrame(data1, schema=fg1_schema) \n",
    "# fg2 = spark.createDataFrame(data2, schema=fg2_schema) \n",
    "# fg3 = spark.createDataFrame(data3, schema=fg3_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3bc310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36779"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"hdfs:///Projects/\" + hdfs.project_name() + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "fg1 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\", header=True, schema=fg1_schema\n",
    ").persist()\n",
    "fg1.count()\n",
    "\n",
    "fg2 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\", header=True, schema=fg2_schema\n",
    ").persist()\n",
    "fg2.count()\n",
    "\n",
    "\n",
    "fg3 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\", header=True, schema=fg3_schema\n",
    ").persist()\n",
    "fg3.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6462b7",
   "metadata": {},
   "source": [
    "## 1. Regular PIT-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "199da4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36779"
     ]
    }
   ],
   "source": [
    "# One feature group\n",
    "\n",
    "# 1. Join the data\n",
    "joined_data = fg1.hint(\"BROADCASTJOIN\").join(\n",
    "    fg2, (fg1.id == fg2.id_2) & (fg1.ts >= fg2.ts_2)\n",
    ").unpersist()\n",
    "\n",
    "# 2. Create window for partitioning and ordering the data\n",
    "win = Window.partitionBy([\"id\", \"ts\"]).orderBy(F.desc(\"ts_2\"))\n",
    "\n",
    "# 3. Rank the rows of each partition\n",
    "ranked_data = joined_data.withColumn(\"rank\", F.rank().over(win)).unpersist()\n",
    "\n",
    "# 4. Take only the columns with rank == 1, for each partition\n",
    "filtered_data = ranked_data.filter(F.col(\"rank\") == 1).unpersist()\n",
    "\n",
    "filtered_data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "833e4c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36779"
     ]
    }
   ],
   "source": [
    "# Two feature groups\n",
    "\n",
    "# 1. Join the data\n",
    "joined_data = fg1.hint(\"BROADCAST\").join(\n",
    "    fg2, (fg1.id == fg2.id_2) & (fg1.ts >= fg2.ts_2)).unpersist().hint(\"BROADCAST\").join(\n",
    "        fg3, (fg1.id == fg3.id_3) & (fg1.ts >= fg3.ts_3)\n",
    ").unpersist()\n",
    "\n",
    "# 2. Create window for partitioning and ordering the data\n",
    "win = Window.partitionBy([\"id\", \"ts\"]).orderBy(F.desc(\"ts_2\"), F.desc(\"ts_3\"))\n",
    "\n",
    "# 3. Rank the rows of each partition\n",
    "ranked_data = joined_data.withColumn(\"rank\", F.rank().over(win)).unpersist()\n",
    "\n",
    "# 4. Take only the columns with rank == 1, for each partition\n",
    "filtered_data = ranked_data.filter(F.col(\"rank\") == 1).unpersist()\n",
    "\n",
    "filtered_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d4f19",
   "metadata": {},
   "source": [
    "# 2. Pre-sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dec0c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36779"
     ]
    }
   ],
   "source": [
    "# Simulate pre-sorting of data\n",
    "sorted_fg1 = fg1.orderBy(F.asc(\"id\")).persist()\n",
    "sorted_fg2 = fg2.orderBy(F.asc(\"id_2\")).persist()\n",
    "sorted_fg3 = fg3.orderBy(F.asc(\"id_3\")).persist()\n",
    "\n",
    "sorted_fg1.count()\n",
    "sorted_fg2.count()\n",
    "sorted_fg3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d156b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1040|   f1|\n",
      "|98162|1060|   f1|\n",
      "|98162|1080|   f1|\n",
      "|98162|1100|   f1|\n",
      "|98162|1120|   f1|\n",
      "|98162|1140|   f1|\n",
      "|98162|1160|   f1|\n",
      "|98162|1180|   f1|\n",
      "|98162|1200|   f1|\n",
      "|98162|1220|   f1|\n",
      "|98162|1240|   f1|\n",
      "|98162|1260|   f1|\n",
      "|98162|1280|   f1|\n",
      "|98162|1300|   f1|\n",
      "|98162|1320|   f1|\n",
      "|98162|1340|   f1|\n",
      "|98162|1360|   f1|\n",
      "|98162|1380|   f1|\n",
      "|98162|1400|   f1|\n",
      "|98163|1020|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sorted_fg1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e49141b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 12)\n",
      "  File \"<stdin>\", line 12\n",
      "    ranked_data = joined_data.withColumn(\"rank\", F.rank().over(win))unpersist()\n",
      "                                                                    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One feature group\n",
    "\n",
    "# 1. Join the data\n",
    "joined_data = fg1.hint(\"MERGE\").join(\n",
    "    fg2, (fg1.id == fg2.id_2) & (fg1.ts >= fg2.ts_2)\n",
    ").unpersist()\n",
    "\n",
    "# 2. Create window for partitioning and ordering the data\n",
    "win = Window.partitionBy([\"id\", \"ts\"]).orderBy(F.desc(\"ts_2\"))\n",
    "\n",
    "# 3. Rank the rows of each partition\n",
    "ranked_data = joined_data.withColumn(\"rank\", F.rank().over(win)).unpersist()\n",
    "\n",
    "# 4. Take only the columns with rank == 1, for each partition\n",
    "filtered_data = ranked_data.filter(F.col(\"rank\") == 1).unpersist()\n",
    "\n",
    "filtered_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64970f27",
   "metadata": {},
   "source": [
    "Preliminary results: Decrease from 9s to 6s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}