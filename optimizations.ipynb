{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e6c8ff",
   "metadata": {},
   "source": [
    "# Optimizations for PIT-joins\n",
    "\n",
    "This notebook will consist of several optimizations for the existing join method. Stuff that will be looked into is the unoptimized PIT-join as well as optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ade68",
   "metadata": {},
   "source": [
    "## 0. Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c239683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>21</td><td>application_1642582607798_0020</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1642582607798_0020/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e03_1642582607798_0020_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row, SparkSession, Window, SQLContext\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col\n",
    "from pyspark.sql import functions as F\n",
    "from hops import hdfs as hdfs\n",
    "from sparkmeasure import StageMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c479aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[1, 5, \"1x\"],\n",
    "         [1, 7, \"1y\"],\n",
    "         [1, 4, \"1z\"],\n",
    "         [2, 6, \"2x\"],\n",
    "         [2, 8, \"2y\"]]\n",
    "\n",
    "data2 = [[1, 5, \"1x\"],\n",
    "         [1, 7, \"1y\"],\n",
    "         [1, 4, \"1z\"],\n",
    "         [2, 6, \"2x\"],\n",
    "         [2, 8, \"2y\"]]\n",
    "\n",
    "data3 = [[1, 10, \"f3-1-10\"],\n",
    "         [1, 1, \"f3-1-1\"],\n",
    "         [1, 6, \"f3-1-6\"],\n",
    "         [2, 2, \"f3-2-2\"],\n",
    "         [2, 8, \"f3-2-8\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cf71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"Three Way PIT\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sql_context = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6275aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5bafcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Used for storing of tables\n",
    "PROJECT_NAME = hdfs.project_name()\n",
    "spark.sql(\"use \" + PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c65eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg1_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), False),\n",
    "  StructField(\"ts\", IntegerType(), False),\n",
    "  StructField(\"label\", StringType(), False)    \n",
    "])\n",
    "\n",
    "fg2_schema = StructType([\n",
    "  StructField(\"id_2\", IntegerType(), False),\n",
    "  StructField(\"ts_2\", IntegerType(), False),\n",
    "  StructField(\"f2\", StringType(), False)\n",
    "])\n",
    "\n",
    "fg3_schema = StructType([\n",
    "  StructField(\"id_3\", IntegerType(), False),\n",
    "  StructField(\"ts_3\", IntegerType(), False),\n",
    "  StructField(\"f3\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35fc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fg1 = spark.createDataFrame(data1, schema=fg1_schema) \n",
    "# fg2 = spark.createDataFrame(data2, schema=fg2_schema) \n",
    "# fg3 = spark.createDataFrame(data3, schema=fg3_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6706587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36779"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"hdfs:///Projects/\" + hdfs.project_name() + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "fg1 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\", header=True, schema=fg1_schema\n",
    ").sort(F.desc(\"ts\")).persist()\n",
    "fg1.count()\n",
    "\n",
    "fg2 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\", header=True, schema=fg2_schema\n",
    ").sort(F.desc(\"ts_2\")).persist()\n",
    "fg2.count()\n",
    "\n",
    "\n",
    "fg3 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\", header=True, schema=fg3_schema\n",
    ").sort(F.desc(\"ts_3\")).persist()\n",
    "fg3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6e8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1400|   f1|\n",
      "|98163|1400|   f1|\n",
      "|98164|1400|   f1|\n",
      "|98165|1400|   f1|\n",
      "|98166|1400|   f1|\n",
      "|98167|1400|   f1|\n",
      "|98168|1400|   f1|\n",
      "|98169|1400|   f1|\n",
      "|98170|1400|   f1|\n",
      "|98171|1400|   f1|\n",
      "|98172|1400|   f1|\n",
      "|98173|1400|   f1|\n",
      "|98174|1400|   f1|\n",
      "|98175|1400|   f1|\n",
      "|98176|1400|   f1|\n",
      "|98177|1400|   f1|\n",
      "|98178|1400|   f1|\n",
      "|98179|1400|   f1|\n",
      "|98180|1400|   f1|\n",
      "|98181|1400|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "fg1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc1395",
   "metadata": {},
   "source": [
    "# 1. Regular PIT-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52eb58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pit_join(join_hint, label_data, *fgs):\n",
    "    # 1. Join the data\n",
    "    joined_data = label_data\n",
    "    for i, fg in enumerate(fgs):\n",
    "        fg_id = \"id_{}\".format(i + 2)\n",
    "        fg_ts = \"ts_{}\".format(i + 2)\n",
    "        joined_data = joined_data.hint(join_hint).join(\n",
    "            fg, (label_data.id == getattr(fg, fg_id)) & (label_data.ts >= getattr(fg, fg_ts))\n",
    "        ).unpersist()\n",
    "\n",
    "    # 2. Create window for partitioning and ordering the data\n",
    "    order_by_param = [F.desc(\"ts_{}\".format(i + 2)) for i in range(len(fgs))]\n",
    "    win = Window.partitionBy([\"id\", \"ts\"]).orderBy(*order_by_param)\n",
    "\n",
    "    # 3. Rank the rows of each partition\n",
    "    ranked_data = joined_data.withColumn(\"rank\", F.rank().over(win)).unpersist()\n",
    "\n",
    "    # 4. Take only the columns with rank == 1, for each partition\n",
    "    filtered_data = ranked_data.filter(F.col(\"rank\") == 1).unpersist()\n",
    "    \n",
    "    return filtered_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee2db995",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_RUNS = 1\n",
    "HINTS = [\"BROADCAST\", \"MERGE\"]\n",
    "\n",
    "def experiment(label_data, *fgs):\n",
    "    for hint in HINTS:\n",
    "        print(\"Running with {}\".format(hint))\n",
    "        for run in range(NO_RUNS):\n",
    "            stagemetrics.begin()\n",
    "            pit_join(hint, label_data, *fgs)\n",
    "            stagemetrics.end()\n",
    "            # stagemetrics.print_report()\n",
    "        df = stagemetrics.create_stagemetrics_DF(\"PerfStageMetrics\").limit(NO_RUNS)\n",
    "        df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "772b1176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BROADCAST\n",
      "+-----+--------------------+-------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
      "|jobId|            jobGroup|stageId|                name|submissionTime|completionTime|stageDuration|numTasks|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleFetchWaitTime|shuffleTotalBytesRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleRecordsRead|shuffleWriteTime|shuffleBytesWritten|shuffleRecordsWritten|\n",
      "+-----+--------------------+-------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
      "|   19|7ece386e-22f5-4d6...|     40|$anonfun$withThre...| 1643620571374| 1643620571716|          342|      21|             88|             93|                     89|                        86|                      0|        0|    244997|               0|                 0|                  0|         20|    48656|             0|           0|                   0|                    0|                        0|                        0|                         0|                    0|                     0|                           0|                 0|               0|                  0|                    0|\n",
      "+-----+--------------------+-------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
      "\n",
      "Running with MERGE\n",
      "+-----+--------+-------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
      "|jobId|jobGroup|stageId|                name|submissionTime|completionTime|stageDuration|numTasks|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleFetchWaitTime|shuffleTotalBytesRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleRecordsRead|shuffleWriteTime|shuffleBytesWritten|shuffleRecordsWritten|\n",
      "+-----+--------+-------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
      "|   21|      33|     47|count at NativeMe...| 1643620580558| 1643620582100|         1542|      21|           1302|           1191|                     83|                        78|                      0|       30|     71523|               0|                 0|                  0|         20|    48656|             0|           0|                   0|                    0|                        0|                        0|                         0|                    0|                     0|                           0|                 0|             801|             406324|                36779|\n",
      "+-----+--------+-------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+"
     ]
    }
   ],
   "source": [
    "# One feature group\n",
    "\n",
    "experiment(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9cce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two feature groups\n",
    "\n",
    "experiment(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eede72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = stagemetrics.create_stagemetrics_DF(\"PerfStageMetrics\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485fde19",
   "metadata": {},
   "source": [
    "# 2. Pre-sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d47ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36779"
     ]
    }
   ],
   "source": [
    "# Simulate pre-sorting of data\n",
    "# Data is already stored pre-sorted\n",
    "DATA_PATH = \"hdfs:///Projects/\" + hdfs.project_name() + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "sorted_fg1 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\", header=True, schema=fg1_schema\n",
    ").persist()\n",
    "fg1.count()\n",
    "\n",
    "sorted_fg2 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\", header=True, schema=fg2_schema\n",
    ").persist()\n",
    "fg2.count()\n",
    "\n",
    "\n",
    "sorted_fg3 = spark.read.csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\", header=True, schema=fg3_schema\n",
    ").persist()\n",
    "fg3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94a19963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1040|   f1|\n",
      "|98162|1060|   f1|\n",
      "|98162|1080|   f1|\n",
      "|98162|1100|   f1|\n",
      "|98162|1120|   f1|\n",
      "|98162|1140|   f1|\n",
      "|98162|1160|   f1|\n",
      "|98162|1180|   f1|\n",
      "|98162|1200|   f1|\n",
      "|98162|1220|   f1|\n",
      "|98162|1240|   f1|\n",
      "|98162|1260|   f1|\n",
      "|98162|1280|   f1|\n",
      "|98162|1300|   f1|\n",
      "|98162|1320|   f1|\n",
      "|98162|1340|   f1|\n",
      "|98162|1360|   f1|\n",
      "|98162|1380|   f1|\n",
      "|98162|1400|   f1|\n",
      "|98163|1020|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sorted_fg1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd05de99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One feature group\n",
    "\n",
    "experiment(sorted_fg1, sorted_fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "991223f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two feature groups\n",
    "\n",
    "experiment(sorted_fg1, sorted_fg2, sorted_fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f532869",
   "metadata": {},
   "source": [
    "# 3. Pre-partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "362a3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id_2\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id_3\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431045c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed_fg1 = spark.table(\"fg1_bucketed\").persist()\n",
    "bucketed_fg2 = spark.table(\"fg2_bucketed\").persist()\n",
    "bucketed_fg3 = spark.table(\"fg3_bucketed\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b45e5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One feature group\n",
    "\n",
    "experiment(bucketed_fg1, bucketed_fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832f5d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two feature groups\n",
    "\n",
    "experiment(bucketed_fg1, bucketed_fg2, bucketed_fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d0fb0",
   "metadata": {},
   "source": [
    "# 4. Pre-partitioning and pre-sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda879f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parition the data based on id\n",
    "sorted_fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "sorted_fg2.write.mode(\"overwrite\").bucketBy(4, \"id_2\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "sorted_fg3.write.mode(\"overwrite\").bucketBy(4, \"id_3\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4d6d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bucketed_fg1 = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "sorted_bucketed_fg2 = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "sorted_bucketed_fg3 = spark.table(\"fg3_bucketed_sorted\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f95db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One feature group\n",
    "\n",
    "experiment(sorted_bucketed_fg1, sorted_bucketed_fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "714a0ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'stagemetrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in experiment\n",
      "NameError: name 'stagemetrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Two feature groups\n",
    "\n",
    "experiment(sorted_bucketed_fg1, sorted_bucketed_fg2, sorted_bucketed_fg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae730d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}