{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7bcd0c",
   "metadata": {},
   "source": [
    "# Optimizations for PIT-joins\n",
    "\n",
    "This notebook will consist of several optimizations for the existing join method. Stuff that will be looked into is the unoptimized PIT-join as well as optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d600739",
   "metadata": {},
   "source": [
    "# 0. Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import io.hops.util.Hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io.github.ackuq._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fg1_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, false),\n",
    "  StructField(\"ts\", IntegerType, false),\n",
    "  StructField(\"label\", StringType, false)    \n",
    "))\n",
    "\n",
    "val fg2_schema = StructType(Array(\n",
    "  StructField(\"id_2\", IntegerType, false),\n",
    "  StructField(\"ts_2\", IntegerType, false),\n",
    "  StructField(\"f2\", StringType, false)    \n",
    "))\n",
    "\n",
    "val fg3_schema = StructType(Array(\n",
    "  StructField(\"id_3\", IntegerType, false),\n",
    "  StructField(\"ts_3\", IntegerType, false),\n",
    "  StructField(\"f3\", StringType, false)    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62800c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1279e9e1\n"
     ]
    }
   ],
   "source": [
    "val spark = SparkSession.builder().master(\"local\").appName(\"PIT Optimizations Scala\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806ced3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use \" + Hops.getProjectName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763e1fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data2: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data3: Seq[org.apache.spark.sql.Row] = List([1,1,f3-1-1], [2,2,f3-2-2], [1,6,f3-1-6], [2,8,f3-2-8], [1,10,f3-1-10])\n"
     ]
    }
   ],
   "source": [
    "val data1 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data2 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data3 = Seq(\n",
    "    Row(1, 1, \"f3-1-1\"),\n",
    "    Row(2, 2, \"f3-2-2\"),\n",
    "    Row(1, 6, \"f3-1-6\"),\n",
    "    Row(2, 8, \"f3-2-8\"),\n",
    "    Row(1, 10, \"f3-1-10\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f16883",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val fg1 = spark.createDataFrame(spark.sparkContext.parallelize(data1), schema=fg1_schema) \n",
    "val fg2 = spark.createDataFrame(spark.sparkContext.parallelize(data2), schema=fg2_schema) \n",
    "val fg3 = spark.createDataFrame(spark.sparkContext.parallelize(data3), schema=fg3_schema) \n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c39ffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: String = hdfs:///Projects/demo_fs_meb10000/Jupyter/PIT-joins/example-data\n",
      "fg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res10: Long = 36779\n",
      "fg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "res12: Long = 36779\n",
      "fg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n",
      "res14: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val DATA_PATH = \"hdfs:///Projects/\" + Hops.getProjectName + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "\n",
    "val fg1 = spark.read.option(\"header\", true).schema(fg1_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg1.count()\n",
    "\n",
    "val fg2 = spark.read.option(\"header\", true).schema(fg2_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts_2\")).persist()\n",
    "fg2.count()\n",
    "\n",
    "val fg3 = spark.read.option(\"header\", true).schema(fg3_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts_3\")).persist()\n",
    "fg3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a54ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1400|   f1|\n",
      "|98163|1400|   f1|\n",
      "|98164|1400|   f1|\n",
      "|98165|1400|   f1|\n",
      "|98166|1400|   f1|\n",
      "|98167|1400|   f1|\n",
      "|98168|1400|   f1|\n",
      "|98169|1400|   f1|\n",
      "|98170|1400|   f1|\n",
      "|98171|1400|   f1|\n",
      "|98172|1400|   f1|\n",
      "|98173|1400|   f1|\n",
      "|98174|1400|   f1|\n",
      "|98175|1400|   f1|\n",
      "|98176|1400|   f1|\n",
      "|98177|1400|   f1|\n",
      "|98178|1400|   f1|\n",
      "|98179|1400|   f1|\n",
      "|98180|1400|   f1|\n",
      "|98181|1400|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fg1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b79e0",
   "metadata": {},
   "source": [
    "# 1. Regular PIT-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d227cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitJoin: (joinHint: String, labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)Long\n"
     ]
    }
   ],
   "source": [
    "def pitJoin(joinHint: String, labelData: DataFrame, fgs: DataFrame*) : Long = {\n",
    "    var joinedData = labelData\n",
    "    for ((fg, i) <- fgs.zipWithIndex) {\n",
    "        val id = i + 2\n",
    "        val fg_id = s\"id_${id}\"\n",
    "        val fg_ts = s\"ts_${id}\"\n",
    "        joinedData = joinedData.hint(joinHint).join(\n",
    "            fg, (labelData(\"id\") === fg(fg_id)) && (labelData(\"ts\") >= fg(fg_ts))\n",
    "        ).unpersist()\n",
    "    }\n",
    "    // 2. Create window for partitioning and ordering the data\n",
    "    val orderByParams = \n",
    "        for (i <- 0 to fgs.length - 1)\n",
    "        yield desc(s\"ts_${i + 2}\")\n",
    "\n",
    "    val win = Window.partitionBy(\"id\", \"ts\").orderBy(orderByParams :_*)\n",
    "\n",
    "    // 3. Rank the rows of each partition\n",
    "    val rankedData = joinedData.withColumn(\"rank\", rank().over(win))\n",
    "\n",
    "    // 4. Take only the columns with rank == 1, for each partition\n",
    "    val filteredData = rankedData.filter(col(\"rank\") === 1)\n",
    "    \n",
    "    filteredData.count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53324b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_RUNS: Int = 10\n",
      "HINTS: List[String] = List(BROADCAST, MERGE)\n",
      "experiment: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)Unit\n"
     ]
    }
   ],
   "source": [
    "val NO_RUNS = 10\n",
    "val HINTS = List(\"BROADCAST\", \"MERGE\")\n",
    "\n",
    "def experiment(labelData: DataFrame, fgs: DataFrame*) : Unit = {\n",
    "    for (hint <- HINTS) {\n",
    "        println(s\"Running with ${hint}\")\n",
    "        for (run <- 0 to NO_RUNS) {\n",
    "            spark.time(pitJoin(hint, labelData, fgs :_*))\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87df6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BROADCAST\n",
      "Time taken: 6182 ms\n",
      "Time taken: 5758 ms\n",
      "Time taken: 5768 ms\n",
      "Time taken: 5716 ms\n",
      "Time taken: 5636 ms\n",
      "Time taken: 5824 ms\n",
      "Time taken: 5685 ms\n",
      "Time taken: 5849 ms\n",
      "Time taken: 5781 ms\n",
      "Time taken: 5730 ms\n",
      "Time taken: 5734 ms\n",
      "Running with MERGE\n",
      "Time taken: 7362 ms\n",
      "Time taken: 7384 ms\n",
      "Time taken: 7334 ms\n",
      "Time taken: 7395 ms\n",
      "Time taken: 7476 ms\n",
      "Time taken: 7370 ms\n",
      "Time taken: 7247 ms\n",
      "Time taken: 7235 ms\n",
      "Time taken: 7601 ms\n",
      "Time taken: 7206 ms\n",
      "Time taken: 7248 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4744a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BROADCAST\n",
      "Time taken: 16257 ms\n",
      "Time taken: 15873 ms\n",
      "Time taken: 15984 ms\n",
      "Time taken: 16264 ms\n",
      "Time taken: 16179 ms\n",
      "Time taken: 16167 ms\n",
      "Time taken: 16028 ms\n",
      "Time taken: 16160 ms\n",
      "Time taken: 15817 ms\n",
      "Time taken: 16062 ms\n",
      "Time taken: 16138 ms\n",
      "Running with MERGE\n",
      "Time taken: 13060 ms\n",
      "Time taken: 13130 ms\n",
      "Time taken: 12974 ms\n",
      "Time taken: 12978 ms\n",
      "Time taken: 13154 ms\n",
      "Time taken: 13212 ms\n",
      "Time taken: 13011 ms\n",
      "Time taken: 13149 ms\n",
      "Time taken: 13148 ms\n",
      "Time taken: 13395 ms\n",
      "Time taken: 13880 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51b68f",
   "metadata": {},
   "source": [
    "# 2. Pre-sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb39d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sortedFg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res26: Long = 36779\n",
      "sortedFg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "res28: Long = 36779\n",
      "sortedFg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n",
      "res30: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "// Simulate pre-sorting of data\n",
    "// Data is already stored pre-sorted\n",
    "\n",
    "val sortedFg1 = spark.read.option(\"header\", true).schema(fg1_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\"\n",
    ").persist()\n",
    "sortedFg1.count()\n",
    "\n",
    "val sortedFg2 = spark.read.option(\"header\", true).schema(fg2_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").persist()\n",
    "sortedFg2.count()\n",
    "\n",
    "val sortedFg3 = spark.read.option(\"header\", true).schema(fg3_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").persist()\n",
    "sortedFg3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d73a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 4174 ms\n",
      "Time taken: 3861 ms\n",
      "Time taken: 3915 ms\n",
      "Time taken: 3854 ms\n",
      "Time taken: 3903 ms\n",
      "Time taken: 3908 ms\n",
      "Time taken: 3910 ms\n",
      "Time taken: 3864 ms\n",
      "Time taken: 3885 ms\n",
      "Time taken: 3845 ms\n",
      "Time taken: 3918 ms\n",
      "Running with MERGE\n",
      "Time taken: 4291 ms\n",
      "Time taken: 4187 ms\n",
      "Time taken: 4198 ms\n",
      "Time taken: 4179 ms\n",
      "Time taken: 4328 ms\n",
      "Time taken: 4394 ms\n",
      "Time taken: 4076 ms\n",
      "Time taken: 4235 ms\n",
      "Time taken: 4123 ms\n",
      "Time taken: 4300 ms\n",
      "Time taken: 4251 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(sortedFg1, sortedFg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ce58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 10444 ms\n",
      "Time taken: 10223 ms\n",
      "Time taken: 10047 ms\n",
      "Time taken: 10362 ms\n",
      "Time taken: 10170 ms\n",
      "Time taken: 10256 ms\n",
      "Time taken: 10098 ms\n",
      "Time taken: 10835 ms\n",
      "Time taken: 9975 ms\n",
      "Time taken: 10309 ms\n",
      "Time taken: 10117 ms\n",
      "Running with MERGE\n",
      "Time taken: 8209 ms\n",
      "Time taken: 8336 ms\n",
      "Time taken: 8580 ms\n",
      "Time taken: 8636 ms\n",
      "Time taken: 8490 ms\n",
      "Time taken: 8252 ms\n",
      "Time taken: 8108 ms\n",
      "Time taken: 8328 ms\n",
      "Time taken: 8402 ms\n",
      "Time taken: 8326 ms\n",
      "Time taken: 8290 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(sortedFg1, sortedFg2, sortedFg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16107548",
   "metadata": {},
   "source": [
    "# 3. Pre-partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dad9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id_2\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id_3\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95327295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketedFg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "bucketedFg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "bucketedFg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val bucketedFg1 = spark.table(\"fg1_bucketed\").persist()\n",
    "val bucketedFg2 = spark.table(\"fg2_bucketed\").persist()\n",
    "val bucketedFg3 = spark.table(\"fg3_bucketed\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c530be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 9280 ms\n",
      "Time taken: 651 ms\n",
      "Time taken: 721 ms\n",
      "Time taken: 630 ms\n",
      "Time taken: 681 ms\n",
      "Time taken: 666 ms\n",
      "Time taken: 623 ms\n",
      "Time taken: 634 ms\n",
      "Time taken: 695 ms\n",
      "Time taken: 753 ms\n",
      "Time taken: 667 ms\n",
      "Running with MERGE\n",
      "Time taken: 1030 ms\n",
      "Time taken: 596 ms\n",
      "Time taken: 529 ms\n",
      "Time taken: 526 ms\n",
      "Time taken: 533 ms\n",
      "Time taken: 561 ms\n",
      "Time taken: 563 ms\n",
      "Time taken: 544 ms\n",
      "Time taken: 594 ms\n",
      "Time taken: 626 ms\n",
      "Time taken: 549 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(bucketedFg1, bucketedFg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5820b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 8131 ms\n",
      "Time taken: 4114 ms\n",
      "Time taken: 4154 ms\n",
      "Time taken: 4131 ms\n",
      "Time taken: 4090 ms\n",
      "Time taken: 4117 ms\n",
      "Time taken: 4428 ms\n",
      "Time taken: 4269 ms\n",
      "Time taken: 4274 ms\n",
      "Time taken: 4157 ms\n",
      "Time taken: 4149 ms\n",
      "Running with MERGE\n",
      "Time taken: 4566 ms\n",
      "Time taken: 4024 ms\n",
      "Time taken: 3929 ms\n",
      "Time taken: 3912 ms\n",
      "Time taken: 3828 ms\n",
      "Time taken: 3907 ms\n",
      "Time taken: 3894 ms\n",
      "Time taken: 3760 ms\n",
      "Time taken: 3894 ms\n",
      "Time taken: 3731 ms\n",
      "Time taken: 3920 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(bucketedFg1, bucketedFg2, bucketedFg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbf7de",
   "metadata": {},
   "source": [
    "# 4.Pre-partitioning and pre-sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e764df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "sortedFg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "sortedFg2.write.mode(\"overwrite\").bucketBy(4, \"id_2\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "sortedFg3.write.mode(\"overwrite\").bucketBy(4, \"id_3\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7851e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sortedBucketedFg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "sortedBucketedFg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_2: int, ts_2: int ... 1 more field]\n",
      "sortedBucketedFg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_3: int, ts_3: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val sortedBucketedFg1 = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "val sortedBucketedFg2 = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "val sortedBucketedFg3 = spark.table(\"fg3_bucketed_sorted\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc4e816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 1314 ms\n",
      "Time taken: 512 ms\n",
      "Time taken: 554 ms\n",
      "Time taken: 541 ms\n",
      "Time taken: 521 ms\n",
      "Time taken: 539 ms\n",
      "Time taken: 545 ms\n",
      "Time taken: 618 ms\n",
      "Time taken: 606 ms\n",
      "Time taken: 504 ms\n",
      "Time taken: 533 ms\n",
      "Running with MERGE\n",
      "Time taken: 492 ms\n",
      "Time taken: 541 ms\n",
      "Time taken: 540 ms\n",
      "Time taken: 548 ms\n",
      "Time taken: 534 ms\n",
      "Time taken: 607 ms\n",
      "Time taken: 517 ms\n",
      "Time taken: 505 ms\n",
      "Time taken: 568 ms\n",
      "Time taken: 528 ms\n",
      "Time taken: 569 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experiment(sortedBucketedFg1, sortedBucketedFg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa6c4655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with BORADCAST\n",
      "Time taken: 4231 ms\n",
      "Time taken: 3863 ms\n",
      "Time taken: 3765 ms\n",
      "Time taken: 3822 ms\n",
      "Time taken: 3759 ms\n",
      "Time taken: 3879 ms\n",
      "Time taken: 3837 ms\n",
      "Time taken: 3841 ms\n",
      "Time taken: 3872 ms\n",
      "Time taken: 3886 ms\n",
      "Time taken: 3831 ms\n",
      "Running with MERGE\n",
      "Time taken: 3930 ms\n",
      "Time taken: 3647 ms\n",
      "Time taken: 3841 ms\n",
      "Time taken: 3727 ms\n",
      "Time taken: 3859 ms\n",
      "Time taken: 3931 ms\n",
      "Time taken: 3844 ms\n",
      "Time taken: 3825 ms\n",
      "Time taken: 3976 ms\n",
      "Time taken: 3764 ms\n",
      "Time taken: 3851 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experiment(sortedBucketedFg1, sortedBucketedFg2, sortedBucketedFg3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}