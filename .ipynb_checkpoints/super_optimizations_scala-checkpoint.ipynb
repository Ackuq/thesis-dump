{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882e6bc6",
   "metadata": {},
   "source": [
    "# Optimizations for PIT-joins\n",
    "\n",
    "This notebook will consist of several optimizations for the existing join method. Stuff that will be looked into is the unoptimized PIT-join as well as optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157cf2ea",
   "metadata": {},
   "source": [
    "# 0. Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ccb1072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>35</td><td>application_1642582607798_0034</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1642582607798_0034/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_e03_1642582607798_0034_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
      "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.expressions.Window\n",
      "import io.hops.util.Hops\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.types.{StructType, IntegerType, StringType, StructField}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import io.hops.util.Hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f79c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import io.github.ackuq.pit.{EarlyStopSortMerge, UnionAsOf}\n"
     ]
    }
   ],
   "source": [
    "import io.github.ackuq.pit.{EarlyStopSortMerge, UnionAsOf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a333e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(ts,IntegerType,true), StructField(label,StringType,true))\n",
      "fg2_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(ts,IntegerType,true), StructField(f2,StringType,true))\n",
      "fg3_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(ts,IntegerType,true), StructField(f3,StringType,true))\n"
     ]
    }
   ],
   "source": [
    "val fg1_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"ts\", IntegerType, true),\n",
    "  StructField(\"label\", StringType, true)    \n",
    "))\n",
    "\n",
    "val fg2_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"ts\", IntegerType, true),\n",
    "  StructField(\"f2\", StringType, true)    \n",
    "))\n",
    "\n",
    "val fg3_schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\"ts\", IntegerType, true),\n",
    "  StructField(\"f3\", StringType, true)    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8469e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@87d8deb\n"
     ]
    }
   ],
   "source": [
    "val spark: SparkSession = SparkSession.builder().master(\"local\").appName(\"PIT Optimizations Scala\").config(\"spark.sql.adaptive.enabled\", true).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ef0c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use \" + Hops.getProjectName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f547a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopSortMerge.init(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5652be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data2: Seq[org.apache.spark.sql.Row] = List([1,4,1z], [1,5,1x], [2,6,2x], [1,7,1y], [2,8,2y])\n",
      "data3: Seq[org.apache.spark.sql.Row] = List([1,1,f3-1-1], [2,2,f3-2-2], [1,6,f3-1-6], [2,8,f3-2-8], [1,10,f3-1-10])\n"
     ]
    }
   ],
   "source": [
    "val data1 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data2 = Seq(\n",
    "    Row(1, 4, \"1z\"),\n",
    "    Row(1, 5, \"1x\"),\n",
    "    Row(2, 6, \"2x\"),\n",
    "    Row(1, 7, \"1y\"),\n",
    "    Row(2, 8, \"2y\")\n",
    ")\n",
    "\n",
    "val data3 = Seq(\n",
    "    Row(1, 1, \"f3-1-1\"),\n",
    "    Row(2, 2, \"f3-2-2\"),\n",
    "    Row(1, 6, \"f3-1-6\"),\n",
    "    Row(2, 8, \"f3-2-8\"),\n",
    "    Row(1, 10, \"f3-1-10\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e52a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val fg1 = spark.createDataFrame(spark.sparkContext.parallelize(data1), schema=fg1_schema) \n",
    "val fg2 = spark.createDataFrame(spark.sparkContext.parallelize(data2), schema=fg2_schema) \n",
    "val fg3 = spark.createDataFrame(spark.sparkContext.parallelize(data3), schema=fg3_schema) \n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80579cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: String = hdfs:///Projects/demo_fs_meb10000/Jupyter/PIT-joins/example-data\n",
      "fg1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res11: Long = 36779\n",
      "fg2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res13: Long = 36779\n",
      "fg3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res15: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val DATA_PATH = \"hdfs:///Projects/\" + Hops.getProjectName + \"/Jupyter/PIT-joins/example-data\"\n",
    "\n",
    "\n",
    "val fg1 = spark.read.option(\"header\", true).schema(fg1_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-1-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg1.count()\n",
    "\n",
    "val fg2 = spark.read.option(\"header\", true).schema(fg2_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg2.count()\n",
    "\n",
    "val fg3 = spark.read.option(\"header\", true).schema(fg3_schema).csv(\n",
    "    DATA_PATH + \"/100000-20-2-out.csv\"\n",
    ").sort(desc(\"ts\")).persist()\n",
    "fg3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6381b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1400|   f1|\n",
      "|98163|1400|   f1|\n",
      "|98164|1400|   f1|\n",
      "|98165|1400|   f1|\n",
      "|98166|1400|   f1|\n",
      "|98167|1400|   f1|\n",
      "|98168|1400|   f1|\n",
      "|98169|1400|   f1|\n",
      "|98170|1400|   f1|\n",
      "|98171|1400|   f1|\n",
      "|98172|1400|   f1|\n",
      "|98173|1400|   f1|\n",
      "|98174|1400|   f1|\n",
      "|98175|1400|   f1|\n",
      "|98176|1400|   f1|\n",
      "|98177|1400|   f1|\n",
      "|98178|1400|   f1|\n",
      "|98179|1400|   f1|\n",
      "|98180|1400|   f1|\n",
      "|98181|1400|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fg1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90af5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: org.apache.spark.sql.DataFrame = [id: int, ts: int ... 1 more field]\n",
      "+-----+----+-----+\n",
      "|   id|  ts|label|\n",
      "+-----+----+-----+\n",
      "|98162|1400|   f1|\n",
      "|98163|1400|   f1|\n",
      "|98164|1400|   f1|\n",
      "|98165|1400|   f1|\n",
      "|98166|1400|   f1|\n",
      "|98167|1400|   f1|\n",
      "|98168|1400|   f1|\n",
      "|98169|1400|   f1|\n",
      "|98170|1400|   f1|\n",
      "|98171|1400|   f1|\n",
      "|98172|1400|   f1|\n",
      "|98173|1400|   f1|\n",
      "|98174|1400|   f1|\n",
      "|98175|1400|   f1|\n",
      "|98176|1400|   f1|\n",
      "|98177|1400|   f1|\n",
      "|98178|1400|   f1|\n",
      "|98179|1400|   f1|\n",
      "|98180|1400|   f1|\n",
      "|98181|1400|   f1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val q = fg1.alias(\"left\")\\\n",
    ".select(\"left.*\", \"__right__.*\").\n",
    "\n",
    "q.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac955e",
   "metadata": {},
   "source": [
    "# 1. Union PIT Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ef035",
   "metadata": {},
   "source": [
    "## 1.1. Sorted on timestamps (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b286d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unionPitJoin: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)org.apache.spark.sql.DataFrame\n",
      "+-----+----+-----+------+------+------+------+\n",
      "|   id|  ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+-----+----+-----+------+------+------+------+\n",
      "|98164|1020|   f1|  1020|    f2|  1020|    f2|\n",
      "|98164|1040|   f1|  1040|    f2|  1040|    f2|\n",
      "|98164|1060|   f1|  1060|    f2|  1060|    f2|\n",
      "|98164|1080|   f1|  1080|    f2|  1080|    f2|\n",
      "|98164|1100|   f1|  1100|    f2|  1100|    f2|\n",
      "|98164|1120|   f1|  1120|    f2|  1120|    f2|\n",
      "|98164|1140|   f1|  1140|    f2|  1140|    f2|\n",
      "|98164|1160|   f1|  1160|    f2|  1160|    f2|\n",
      "|98164|1180|   f1|  1180|    f2|  1180|    f2|\n",
      "|98164|1200|   f1|  1200|    f2|  1200|    f2|\n",
      "|98164|1220|   f1|  1220|    f2|  1220|    f2|\n",
      "|98164|1240|   f1|  1240|    f2|  1240|    f2|\n",
      "|98164|1260|   f1|  1260|    f2|  1260|    f2|\n",
      "|98164|1280|   f1|  1280|    f2|  1280|    f2|\n",
      "|98164|1300|   f1|  1300|    f2|  1300|    f2|\n",
      "|98164|1320|   f1|  1320|    f2|  1320|    f2|\n",
      "|98164|1340|   f1|  1340|    f2|  1340|    f2|\n",
      "|98164|1360|   f1|  1360|    f2|  1360|    f2|\n",
      "|98164|1380|   f1|  1380|    f2|  1380|    f2|\n",
      "|98164|1400|   f1|  1400|    f2|  1400|    f2|\n",
      "+-----+----+-----+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def unionPitJoin(labelData: DataFrame, fgs: DataFrame*) : DataFrame = {\n",
    "    var joinedData = labelData\n",
    "    for ((fg, i) <- fgs.zipWithIndex) {\n",
    "        val num = i + 2\n",
    "        joinedData = UnionAsOf.join(\n",
    "            joinedData,\n",
    "            fg,\n",
    "            rightPrefix = s\"fg${num}_\",\n",
    "            partitionCols = Seq(\"id\")\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    joinedData\n",
    "}\n",
    "\n",
    "unionPitJoin(fg1, fg2, fg3).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6ca962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "+- Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "         +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926, fg2_f2#962, fg2_f2#962]\n",
      "            +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "               +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "                  +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#926, df_combined_ts#935]\n",
      "                     +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925, fg2_ts#953, fg2_ts#953]\n",
      "                        +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                           +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "                              +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935]\n",
      "                                 +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1, id#944, id#944]\n",
      "                                    +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                                       +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1]\n",
      "                                          +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true\n",
      "                                             +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, coalesce(ts#2, fg2_ts#925) AS df_combined_ts#935]\n",
      "                                                +- Union false, false\n",
      "                                                   :- Project [id#1, ts#2, label#3, df_index#889, cast(fg2_ts#894 as int) AS fg2_ts#925, cast(fg2_f2#900 as string) AS fg2_f2#926]\n",
      "                                                   :  +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#894, null AS fg2_f2#900]\n",
      "                                                   :     +- Project [id#1, ts#2, label#3, df_index#889, null AS fg2_ts#894]\n",
      "                                                   :        +- Project [id#1, ts#2, label#3, 1 AS df_index#889]\n",
      "                                                   :           +- Sort [ts#2 DESC NULLS LAST], true\n",
      "                                                   :              +- Relation[id#1,ts#2,label#3] csv\n",
      "                                                   +- Project [id#108, cast(ts#912 as int) AS ts#927, cast(label#918 as string) AS label#928, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                      +- Project [id#108, ts#912, label#918, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                         +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, ts#912, null AS label#918]\n",
      "                                                            +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, null AS ts#912]\n",
      "                                                               +- Project [id#108, fg2_ts#884, fg2_f2#885, 0 AS df_index#907]\n",
      "                                                                  +- Project [id#108, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885]\n",
      "                                                                     +- Sort [ts#109 DESC NULLS LAST], true\n",
      "                                                                        +- Relation[id#108,ts#109,f2#110] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, ts: int, label: string, fg2_ts: int, fg2_f2: string\n",
      "Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "+- Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#962, df_combined_ts#935]\n",
      "         +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926, fg2_f2#962, fg2_f2#962]\n",
      "            +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "               +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "                  +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, fg2_f2#926, df_combined_ts#935]\n",
      "                     +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925, fg2_ts#953, fg2_ts#953]\n",
      "                        +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                           +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "                              +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935]\n",
      "                                 +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1, id#944, id#944]\n",
      "                                    +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                                       +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1]\n",
      "                                          +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true\n",
      "                                             +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, coalesce(ts#2, fg2_ts#925) AS df_combined_ts#935]\n",
      "                                                +- Union false, false\n",
      "                                                   :- Project [id#1, ts#2, label#3, df_index#889, cast(fg2_ts#894 as int) AS fg2_ts#925, cast(fg2_f2#900 as string) AS fg2_f2#926]\n",
      "                                                   :  +- Project [id#1, ts#2, label#3, df_index#889, fg2_ts#894, null AS fg2_f2#900]\n",
      "                                                   :     +- Project [id#1, ts#2, label#3, df_index#889, null AS fg2_ts#894]\n",
      "                                                   :        +- Project [id#1, ts#2, label#3, 1 AS df_index#889]\n",
      "                                                   :           +- Sort [ts#2 DESC NULLS LAST], true\n",
      "                                                   :              +- Relation[id#1,ts#2,label#3] csv\n",
      "                                                   +- Project [id#108, cast(ts#912 as int) AS ts#927, cast(label#918 as string) AS label#928, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                      +- Project [id#108, ts#912, label#918, df_index#907, fg2_ts#884, fg2_f2#885]\n",
      "                                                         +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, ts#912, null AS label#918]\n",
      "                                                            +- Project [id#108, fg2_ts#884, fg2_f2#885, df_index#907, null AS ts#912]\n",
      "                                                               +- Project [id#108, fg2_ts#884, fg2_f2#885, 0 AS df_index#907]\n",
      "                                                                  +- Project [id#108, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885]\n",
      "                                                                     +- Sort [ts#109 DESC NULLS LAST], true\n",
      "                                                                        +- Relation[id#108,ts#109,f2#110] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "+- Filter isnotnull(ts#2)\n",
      "   +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "      +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "         +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "            +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "               +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                  +- Project [ts#2, label#3, df_index#889, fg2_ts#925, fg2_f2#926, df_combined_ts#935, id#1]\n",
      "                     +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true\n",
      "                        +- Union false, false\n",
      "                           :- Project [id#1, ts#2, label#3, 1 AS df_index#889, null AS fg2_ts#925, null AS fg2_f2#926, ts#2 AS df_combined_ts#935]\n",
      "                           :  +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           :        +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "                           :           +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "                           :              +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "                           +- Project [id#108, null AS ts#927, null AS label#928, 0 AS df_index#907, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885, ts#109 AS df_combined_ts#1041]\n",
      "                              +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                    +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                                       +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                                          +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#944, ts#2, label#3, fg2_ts#953, fg2_f2#962]\n",
      "   +- Filter isnotnull(ts#2)\n",
      "      +- Window [last(fg2_f2#926, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#962], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "         +- Project [id#944, ts#2, label#3, df_index#889, fg2_ts#953, df_combined_ts#935, fg2_f2#926]\n",
      "            +- Window [last(fg2_ts#925, true) windowspecdefinition(id#944, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#953], [id#944], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "               +- Sort [id#944 ASC NULLS FIRST, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#944, 200), ENSURE_REQUIREMENTS, [id=#1226]\n",
      "                     +- Project [id#944, ts#2, label#3, df_index#889, fg2_f2#926, df_combined_ts#935, fg2_ts#925]\n",
      "                        +- Window [last(id#1, true) windowspecdefinition(id#1, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#944], [id#1], [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST]\n",
      "                           +- Sort [id#1 ASC NULLS FIRST, df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(id#1, 200), ENSURE_REQUIREMENTS, [id=#1221]\n",
      "                                 +- Sort [df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST], true, 0\n",
      "                                    +- Exchange rangepartitioning(df_combined_ts#935 ASC NULLS FIRST, df_index#889 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#1218]\n",
      "                                       +- Union\n",
      "                                          :- Project [id#1, ts#2, label#3, 1 AS df_index#889, null AS fg2_ts#925, null AS fg2_f2#926, ts#2 AS df_combined_ts#935]\n",
      "                                          :  +- InMemoryTableScan [id#1, label#3, ts#2]\n",
      "                                          :        +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                          :              +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "                                          :                 +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "                                          :                    +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "                                          +- Project [id#108, null AS ts#927, null AS label#928, 0 AS df_index#907, ts#109 AS fg2_ts#884, f2#110 AS fg2_f2#885, ts#109 AS df_combined_ts#1041]\n",
      "                                             +- InMemoryTableScan [f2#110, id#108, ts#109]\n",
      "                                                   +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                         +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                                                            +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                                                               +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2).explain(extended=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caee0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------+------+\n",
      "|   id|  ts|label|fg2_ts|fg2_f2|\n",
      "+-----+----+-----+------+------+\n",
      "|98164|1020|   f1|  1020|    f2|\n",
      "|98164|1040|   f1|  1040|    f2|\n",
      "|98164|1060|   f1|  1060|    f2|\n",
      "|98164|1080|   f1|  1080|    f2|\n",
      "|98164|1100|   f1|  1100|    f2|\n",
      "|98164|1120|   f1|  1120|    f2|\n",
      "|98164|1140|   f1|  1140|    f2|\n",
      "|98164|1160|   f1|  1160|    f2|\n",
      "|98164|1180|   f1|  1180|    f2|\n",
      "|98164|1200|   f1|  1200|    f2|\n",
      "|98164|1220|   f1|  1220|    f2|\n",
      "|98164|1240|   f1|  1240|    f2|\n",
      "|98164|1260|   f1|  1260|    f2|\n",
      "|98164|1280|   f1|  1280|    f2|\n",
      "|98164|1300|   f1|  1300|    f2|\n",
      "|98164|1320|   f1|  1320|    f2|\n",
      "|98164|1340|   f1|  1340|    f2|\n",
      "|98164|1360|   f1|  1360|    f2|\n",
      "|98164|1380|   f1|  1380|    f2|\n",
      "|98164|1400|   f1|  1400|    f2|\n",
      "+-----+----+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "45844b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: org.apache.spark.sql.DataFrame = [id: int, ts: int ... 3 more fields]\n",
      "res187: Long = 36779\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#74683, ts#47522, label#47523, fg2_ts#74692, fg2_f2#74701]\n",
      "   +- Filter isnotnull(ts#47522)\n",
      "      +- Window [last(fg2_f2#74665, true) windowspecdefinition(id#74683, df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_f2#74701], [id#74683], [df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST]\n",
      "         +- Project [id#74683, ts#47522, label#47523, df_index#74628, fg2_ts#74692, df_combined_ts#74674, fg2_f2#74665]\n",
      "            +- Window [last(fg2_ts#74664, true) windowspecdefinition(id#74683, df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS fg2_ts#74692], [id#74683], [df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST]\n",
      "               +- Sort [id#74683 ASC NULLS FIRST, df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#74683, 200), ENSURE_REQUIREMENTS, [id=#47534]\n",
      "                     +- Project [id#74683, ts#47522, label#47523, df_index#74628, fg2_f2#74665, df_combined_ts#74674, fg2_ts#74664]\n",
      "                        +- Window [last(id#47521, true) windowspecdefinition(id#47521, df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#74683], [id#47521], [df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST]\n",
      "                           +- Sort [id#47521 ASC NULLS FIRST, df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(id#47521, 200), ENSURE_REQUIREMENTS, [id=#47529]\n",
      "                                 +- Sort [df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST], true, 0\n",
      "                                    +- Exchange rangepartitioning(df_combined_ts#74674 ASC NULLS FIRST, df_index#74628 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#47526]\n",
      "                                       +- Union\n",
      "                                          :- Project [id#47521, ts#47522, label#47523, 1 AS df_index#74628, null AS fg2_ts#74664, null AS fg2_f2#74665, ts#47522 AS df_combined_ts#74674]\n",
      "                                          :  +- InMemoryTableScan [id#47521, label#47523, ts#47522]\n",
      "                                          :        +- InMemoryRelation [id#47521, ts#47522, label#47523], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                          :              +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "                                          :                 +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "                                          :                    +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "                                          +- Project [id#47610, null AS ts#74666, null AS label#74667, 0 AS df_index#74646, ts#47611 AS fg2_ts#74623, f2#47612 AS fg2_f2#74624, ts#47611 AS df_combined_ts#74896]\n",
      "                                             +- InMemoryTableScan [f2#47612, id#47610, ts#47611]\n",
      "                                                   +- InMemoryRelation [id#47610, ts#47611, f2#47612], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                         +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                                                            +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                                                               +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val query = unionPitJoin(fg1, fg2)\n",
    "query.count\n",
    "query.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "36d5cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_RUNS: Int = 10\n",
      "experimentUnion: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)Unit\n"
     ]
    }
   ],
   "source": [
    "val NO_RUNS = 10\n",
    "\n",
    "def experimentUnion(labelData: DataFrame, fgs: DataFrame*) : Unit = {\n",
    "    for (run <- 1 to NO_RUNS) {\n",
    "        spark.time(unionPitJoin(labelData, fgs :_*).show(0))\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bdb4e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2).show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cb41ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionPitJoin(fg1, fg2, fg3).show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "61ce5ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2014 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2194 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1953 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2086 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1998 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1897 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1969 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2077 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2093 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 2100 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentUnion(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80bbee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13460 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13476 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12831 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13218 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13175 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12331 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13341 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13052 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12976 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13565 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c292be",
   "metadata": {},
   "source": [
    "## 1.2. Sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b3f44d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1Sorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res225: Long = 36779\n",
      "fg2Sorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res226: Long = 36779\n",
      "fg3Sorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res227: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1Sorted = fg1.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg1Sorted.count\n",
    "val fg2Sorted = fg2.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg2Sorted.count\n",
    "val fg3Sorted = fg3.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg3Sorted.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c473223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13640 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13250 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12769 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12907 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12619 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12734 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13362 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 13139 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12711 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12698 ms\n"
     ]
    }
   ],
   "source": [
    "// One feature group\n",
    "\n",
    "experimentUnion(fg1Sorted, fg2Sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2381a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 30103 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 30948 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 29802 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 30798 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 29785 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 31149 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 30351 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 30221 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 31463 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 31170 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1Sorted, fg2Sorted, fg3Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e7b75",
   "metadata": {},
   "source": [
    "## 1.3 Pre-partitioned sorted on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "03b4f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "53c8fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1Bucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res216: Long = 36779\n",
      "fg2Bucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res217: Long = 36779\n",
      "fg3Bucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res218: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1Bucketed = spark.table(\"fg1_bucketed\").persist()\n",
    "fg1Bucketed.count\n",
    "val fg2Bucketed = spark.table(\"fg2_bucketed\").persist()\n",
    "fg2Bucketed.count\n",
    "val fg3Bucketed = spark.table(\"fg3_bucketed\").persist()\n",
    "fg3Bucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e705e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1263 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1356 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1318 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1251 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1285 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1530 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1243 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1379 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1355 ms\n",
      "+---+---+-----+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|\n",
      "+---+---+-----+------+------+\n",
      "+---+---+-----+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 1318 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentUnion(fg1Bucketed, fg2Bucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a16b1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11889 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11878 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11982 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12197 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11478 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12236 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11652 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11829 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 12434 ms\n",
      "+---+---+-----+------+------+------+------+\n",
      "| id| ts|label|fg2_ts|fg2_f2|fg3_ts|fg3_f3|\n",
      "+---+---+-----+------+------+------+------+\n",
      "+---+---+-----+------+------+------+------+\n",
      "only showing top 0 rows\n",
      "\n",
      "Time taken: 11844 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1Bucketed, fg2Bucketed, fg3Bucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f238ad",
   "metadata": {},
   "source": [
    "## 2.4. Pre-partitioned, sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2226ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "fg2Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "fg3Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d75bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res168: Long = 36779\n",
      "fg2SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res169: Long = 36779\n",
      "fg3SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res170: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1SortedBucketed = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "fg1SortedBucketed.count\n",
    "val fg2SortedBucketed = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "fg2SortedBucketed.count\n",
    "val fg3SortedBucketed = spark.table(\"fg3_bucketed_sorted\").persist()\n",
    "fg3SortedBucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6c339a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "<console>:59: error: not found: value fg1SortedBucketed\n",
      "       experimentUnion(fg1SortedBucketed, fg2SortedBucketed)\n",
      "                       ^\n",
      "<console>:59: error: not found: value fg2SortedBucketed\n",
      "       experimentUnion(fg1SortedBucketed, fg2SortedBucketed)\n",
      "                                          ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentUnion(fg1SortedBucketed, fg2SortedBucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dc7def71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "<console>:59: error: not found: value fg1SortedBucketed\n",
      "       experimentUnion(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)\n",
      "                       ^\n",
      "<console>:59: error: not found: value fg2SortedBucketed\n",
      "       experimentUnion(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)\n",
      "                                          ^\n",
      "<console>:59: error: not found: value fg3SortedBucketed\n",
      "       experimentUnion(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)\n",
      "                                                             ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentUnion(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731321b8",
   "metadata": {},
   "source": [
    "# 2. Early stop sort merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c7d64",
   "metadata": {},
   "source": [
    "## 2.1 Sorted on timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b1e8664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earlyStopSortMerge: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)org.apache.spark.sql.DataFrame\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- PITJoin [ts#2], [ts#109], [id#1], [id#108]\n",
      "   :- Sort [id#1 DESC NULLS LAST, ts#2 DESC NULLS LAST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#1, 200), ENSURE_REQUIREMENTS, [id=#22400]\n",
      "   :     +- Filter isnotnull(id#1)\n",
      "   :        +- InMemoryTableScan [id#1, ts#2, label#3], [isnotnull(id#1)]\n",
      "   :              +- InMemoryRelation [id#1, ts#2, label#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                    +- *(1) Sort [ts#2 DESC NULLS LAST], true, 0\n",
      "   :                       +- Exchange rangepartitioning(ts#2 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#10]\n",
      "   :                          +- FileScan csv [id#1,ts#2,label#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,label:string>\n",
      "   +- Sort [id#108 DESC NULLS LAST, ts#109 DESC NULLS LAST], false, 0\n",
      "      +- Exchange hashpartitioning(id#108, 200), ENSURE_REQUIREMENTS, [id=#22401]\n",
      "         +- Filter isnotnull(id#108)\n",
      "            +- InMemoryTableScan [id#108, ts#109, f2#110], [isnotnull(id#108)]\n",
      "                  +- InMemoryRelation [id#108, ts#109, f2#110], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(1) Sort [ts#109 DESC NULLS LAST], true, 0\n",
      "                           +- Exchange rangepartitioning(ts#109 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#49]\n",
      "                              +- FileScan csv [id#108,ts#109,f2#110] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://rpc.namenode.service.consul:8020/Projects/demo_fs_meb10000/Jupyter/PIT-j..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,ts:int,f2:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def earlyStopSortMerge(labelData: DataFrame, fgs: DataFrame*): DataFrame = {\n",
    "    var joinedData = labelData\n",
    "    for (fg <- fgs) {\n",
    "        joinedData = joinedData.join(\n",
    "            fg,\n",
    "            EarlyStopSortMerge.pit(labelData(\"ts\"), fg(\"ts\")) && labelData(\"id\") === fg(\"id\")\n",
    "        )\n",
    "    }\n",
    "    joinedData\n",
    "}\n",
    "\n",
    "earlyStopSortMerge(fg1, fg2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3cb3c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------+----+---+\n",
      "|    id|  ts|label|    id|  ts| f2|\n",
      "+------+----+-----+------+----+---+\n",
      "|100000|1400|   f1|100000|1400| f2|\n",
      "|100000|1380|   f1|100000|1380| f2|\n",
      "|100000|1360|   f1|100000|1360| f2|\n",
      "|100000|1340|   f1|100000|1340| f2|\n",
      "|100000|1320|   f1|100000|1320| f2|\n",
      "|100000|1300|   f1|100000|1300| f2|\n",
      "|100000|1280|   f1|100000|1280| f2|\n",
      "|100000|1260|   f1|100000|1260| f2|\n",
      "|100000|1240|   f1|100000|1240| f2|\n",
      "|100000|1220|   f1|100000|1220| f2|\n",
      "|100000|1200|   f1|100000|1200| f2|\n",
      "|100000|1180|   f1|100000|1180| f2|\n",
      "|100000|1160|   f1|100000|1160| f2|\n",
      "|100000|1140|   f1|100000|1140| f2|\n",
      "|100000|1120|   f1|100000|1120| f2|\n",
      "|100000|1100|   f1|100000|1100| f2|\n",
      "|100000|1080|   f1|100000|1080| f2|\n",
      "|100000|1060|   f1|100000|1060| f2|\n",
      "|100000|1040|   f1|100000|1040| f2|\n",
      "|100000|1020|   f1|100000|1020| f2|\n",
      "+------+----+-----+------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earlyStopSortMerge(fg1, fg2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a87b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3471 ms\n",
      "res155: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "spark.time(earlyStopSortMerge(fg1, fg2).count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "05e3a8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_RUNS: Int = 10\n",
      "experimentEarlyStop: (labelData: org.apache.spark.sql.DataFrame, fgs: org.apache.spark.sql.DataFrame*)Unit\n"
     ]
    }
   ],
   "source": [
    "val NO_RUNS = 10\n",
    "\n",
    "def experimentEarlyStop(labelData: DataFrame, fgs: DataFrame*) : Unit = {\n",
    "    for (run <- 0 to NO_RUNS) {\n",
    "        spark.time(earlyStopSortMerge(labelData, fgs :_*).count)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09215cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2800 ms\n",
      "Time taken: 3001 ms\n",
      "Time taken: 3159 ms\n",
      "Time taken: 2842 ms\n",
      "Time taken: 3011 ms\n",
      "Time taken: 3134 ms\n",
      "Time taken: 2876 ms\n",
      "Time taken: 3055 ms\n",
      "Time taken: 3021 ms\n",
      "Time taken: 3202 ms\n",
      "Time taken: 2957 ms\n"
     ]
    }
   ],
   "source": [
    "// One feature group\n",
    "experimentEarlyStop(fg1, fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6f0728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4279 ms\n",
      "Time taken: 4687 ms\n",
      "Time taken: 4280 ms\n",
      "Time taken: 4377 ms\n",
      "Time taken: 4710 ms\n",
      "Time taken: 4444 ms\n",
      "Time taken: 4444 ms\n",
      "Time taken: 4740 ms\n",
      "Time taken: 4359 ms\n",
      "Time taken: 4361 ms\n",
      "Time taken: 4691 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1, fg2, fg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d5f26",
   "metadata": {},
   "source": [
    "## 2.2. Sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e873df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "incomplete statement\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val fg1Sorted = fg1.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg1Sorted.count\n",
    "val fg2Sorted = fg2.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg2Sorted.count\n",
    "val fg3Sorted = fg3.orderBy(desc(\"id\"), desc(\"ts\")).persist\n",
    "fg3Sorted.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b27d167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 8659 ms\n",
      "Time taken: 8757 ms\n",
      "Time taken: 8641 ms\n",
      "Time taken: 8462 ms\n",
      "Time taken: 8406 ms\n",
      "Time taken: 8406 ms\n",
      "Time taken: 8489 ms\n",
      "Time taken: 8444 ms\n",
      "Time taken: 8475 ms\n",
      "Time taken: 8302 ms\n",
      "Time taken: 8752 ms\n"
     ]
    }
   ],
   "source": [
    "// One feature group\n",
    "\n",
    "experimentEarlyStop(fg1Sorted, fg2Sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4c7cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 13336 ms\n",
      "Time taken: 14551 ms\n",
      "Time taken: 14232 ms\n",
      "Time taken: 13419 ms\n",
      "Time taken: 12934 ms\n",
      "Time taken: 13404 ms\n",
      "Time taken: 13093 ms\n",
      "Time taken: 13280 ms\n",
      "Time taken: 13653 ms\n",
      "Time taken: 13040 ms\n",
      "Time taken: 13113 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1Sorted, fg2Sorted, fg3Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9d17e",
   "metadata": {},
   "source": [
    "## 2.3. Pre-partitioned, sorted on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "459f0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed\")\n",
    "fg2.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed\")\n",
    "fg3.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ca3be8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1Bucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res216: Long = 36779\n",
      "fg2Bucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res217: Long = 36779\n",
      "fg3Bucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res218: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1Bucketed = spark.table(\"fg1_bucketed\").persist()\n",
    "fg1Bucketed.count\n",
    "val fg2Bucketed = spark.table(\"fg2_bucketed\").persist()\n",
    "fg2Bucketed.count\n",
    "val fg3Bucketed = spark.table(\"fg3_bucketed\").persist()\n",
    "fg3Bucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "442a88a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 499 ms\n",
      "Time taken: 270 ms\n",
      "Time taken: 267 ms\n",
      "Time taken: 258 ms\n",
      "Time taken: 240 ms\n",
      "Time taken: 236 ms\n",
      "Time taken: 224 ms\n",
      "Time taken: 213 ms\n",
      "Time taken: 328 ms\n",
      "Time taken: 358 ms\n",
      "Time taken: 251 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentEarlyStop(fg1Bucketed, fg2Bucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e46632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 552 ms\n",
      "Time taken: 356 ms\n",
      "Time taken: 345 ms\n",
      "Time taken: 345 ms\n",
      "Time taken: 336 ms\n",
      "Time taken: 396 ms\n",
      "Time taken: 392 ms\n",
      "Time taken: 310 ms\n",
      "Time taken: 339 ms\n",
      "Time taken: 310 ms\n",
      "Time taken: 334 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1Bucketed, fg2Bucketed, fg3Bucketed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d643d",
   "metadata": {},
   "source": [
    "## 2.4. Pre-partitioned, sorted on id and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "461a3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parition the data based on id\n",
    "fg1Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg1_bucketed_sorted\")\n",
    "fg2Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg2_bucketed_sorted\")\n",
    "fg3Sorted.write.mode(\"overwrite\").bucketBy(4, \"id\").saveAsTable(\"fg3_bucketed_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a10376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg1SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res168: Long = 36779\n",
      "fg2SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res169: Long = 36779\n",
      "fg3SortedBucketed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, ts: int ... 1 more field]\n",
      "res170: Long = 36779\n"
     ]
    }
   ],
   "source": [
    "val fg1SortedBucketed = spark.table(\"fg1_bucketed_sorted\").persist()\n",
    "fg1SortedBucketed.count\n",
    "val fg2SortedBucketed = spark.table(\"fg2_bucketed_sorted\").persist()\n",
    "fg2SortedBucketed.count\n",
    "val fg3SortedBucketed = spark.table(\"fg3_bucketed_sorted\").persist()\n",
    "fg3SortedBucketed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7f14b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 262 ms\n",
      "Time taken: 248 ms\n",
      "Time taken: 238 ms\n",
      "Time taken: 249 ms\n",
      "Time taken: 280 ms\n",
      "Time taken: 283 ms\n",
      "Time taken: 257 ms\n",
      "Time taken: 221 ms\n",
      "Time taken: 249 ms\n",
      "Time taken: 234 ms\n",
      "Time taken: 269 ms\n"
     ]
    }
   ],
   "source": [
    "// One Feature group\n",
    "\n",
    "experimentEarlyStop(fg1SortedBucketed, fg2SortedBucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f447fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 327 ms\n",
      "Time taken: 281 ms\n",
      "Time taken: 305 ms\n",
      "Time taken: 299 ms\n",
      "Time taken: 277 ms\n",
      "Time taken: 287 ms\n",
      "Time taken: 294 ms\n",
      "Time taken: 312 ms\n",
      "Time taken: 313 ms\n",
      "Time taken: 370 ms\n",
      "Time taken: 285 ms\n"
     ]
    }
   ],
   "source": [
    "// Two feature groups\n",
    "\n",
    "experimentEarlyStop(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6255670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------+----+---+------+----+---+\n",
      "|    id|  ts|label|    id|  ts| f2|    id|  ts| f3|\n",
      "+------+----+-----+------+----+---+------+----+---+\n",
      "|100000|1400|   f1|100000|1400| f2|100000|1400| f2|\n",
      "|100000|1380|   f1|100000|1380| f2|100000|1380| f2|\n",
      "|100000|1360|   f1|100000|1360| f2|100000|1360| f2|\n",
      "|100000|1340|   f1|100000|1340| f2|100000|1340| f2|\n",
      "|100000|1320|   f1|100000|1320| f2|100000|1320| f2|\n",
      "|100000|1300|   f1|100000|1300| f2|100000|1300| f2|\n",
      "|100000|1280|   f1|100000|1280| f2|100000|1280| f2|\n",
      "|100000|1260|   f1|100000|1260| f2|100000|1260| f2|\n",
      "|100000|1240|   f1|100000|1240| f2|100000|1240| f2|\n",
      "|100000|1220|   f1|100000|1220| f2|100000|1220| f2|\n",
      "|100000|1200|   f1|100000|1200| f2|100000|1200| f2|\n",
      "|100000|1180|   f1|100000|1180| f2|100000|1180| f2|\n",
      "|100000|1160|   f1|100000|1160| f2|100000|1160| f2|\n",
      "|100000|1140|   f1|100000|1140| f2|100000|1140| f2|\n",
      "|100000|1120|   f1|100000|1120| f2|100000|1120| f2|\n",
      "|100000|1100|   f1|100000|1100| f2|100000|1100| f2|\n",
      "|100000|1080|   f1|100000|1080| f2|100000|1080| f2|\n",
      "|100000|1060|   f1|100000|1060| f2|100000|1060| f2|\n",
      "|100000|1040|   f1|100000|1040| f2|100000|1040| f2|\n",
      "|100000|1020|   f1|100000|1020| f2|100000|1020| f2|\n",
      "+------+----+-----+------+----+---+------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earlyStopSortMerge(fg1SortedBucketed, fg2SortedBucketed, fg3SortedBucketed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903b3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}